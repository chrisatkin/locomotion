Automatically generated by Mendeley 1.9.2
Any changes to this file will be lost if it is regenerated by Mendeley.

@inproceedings{Yamada2003,
abstract = {In this paper, we propose a method for analyzing word-word dependencies using deterministic bottom-up manner using Support Vector machines. We experimented with dependency trees converted from Penn treebank data, and achieved over 90 \% accuracy of word-word dependency. Though the result is little worse than the most up-to-date phrase structure based parsers, it looks satisfactorily accurate considering that our parser uses no information from phrase structures. 1},
author = {Yamada, Hiroyasu and Matsumoto, Yuji},
booktitle = {In Proceedings of IWPT},
keywords = {nlp,nlp parser,svm apps},
pages = {195--206},
title = {{Statistical Dependency Analysis with Support Vector Machines}},
year = {2003}
}
@inproceedings{Orchard2010,
abstract = {A fully automatic, compiler-driven approach to parallelisation can result in unpredictable time and space costs for compiled code. On the other hand, a fully manual approach to parallelisation can be long, tedious, prone to errors, hard to debug, and often architecture- specific. We present a declarative domain-specific language, Yp- nos, for expressing structured grid computations which encourages manual specification of causally sequential operations but then al- lows a simple, predictable, static analysis to generate optimised, parallel implementations. We introduce the language and provide some discussion on the theoretical aspects of the language seman- tics, particularly the structuring of computations around the cate- gory theoretic notion of a comonad.},
address = {New York, New York, USA},
author = {Orchard, Dominic A. and Bolingbroke, Max and Mycroft, Alan},
booktitle = {Proceedings of the 5th ACM SIGPLAN workshop on Declarative aspects of multicore programming - DAMP '10},
doi = {10.1145/1708046.1708053},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Orchard, Bolingbroke, Mycroft - 2010 - Ypnos.pdf:pdf},
isbn = {9781605588599},
keywords = {comonads,edsl,parallelism,stencils,structured grids,ypnos},
month = jan,
pages = {15},
publisher = {ACM Press},
title = {{Ypnos}},
url = {http://dl.acm.org/citation.cfm?id=1708046.1708053 http://portal.acm.org/citation.cfm?doid=1708046.1708053},
year = {2010}
}
@article{Ketterlin,
author = {Ketterlin, Alain and Clauss, Philippe},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Ketterlin, Clauss - Unknown - Transparent Parallelization of Binary Code.pdf:pdf},
keywords = {binary code,polytope model,static parallelization},
title = {{Transparent Parallelization of Binary Code}}
}
@book{Allen2000,
address = {San Francisco},
author = {Allen, Randy and Kennedy, Ken},
edition = {First},
editor = {Penrose, Denise E. M.},
isbn = {1-55860-286-0},
publisher = {Morgan Kaufmann},
title = {{Optimising Compilers for Modern Architectures: A Dependence-based Approach}},
year = {2000}
}
@misc{XeonPhi,
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Intel® Xeon Phi™ Coprocessor Parallel Processing.html:html},
keywords = {croprocessors,inte xeon processors,intel xeon phi coprocessors,xeon phi},
title = {{Intel® Xeon Phi™ Coprocessor: Parallel Processing}},
url = {http://www.intel.ru/content/www/us/en/high-performance-computing/high-performance-xeon-phi-coprocessor-brief.html},
urldate = {29/01/13}
}
@inproceedings{Rogers2005,
abstract = {Increasing the number of instructions executing in parallel has helped improve processor performance, but the technique is limited. Executing code on parallel threads and processors has fewer limitations, but most computer programs tend to be serial in nature. This paper presents a compiler optimisation that at run-time parallelises code inside a JVM and thereby increases the number of threads. We show Spec JVM benchmark results for this optimisation. The performance on a current desktop processor is slower than without parallel threads, caused by thread creation costs, but with these costs removed the performance is better than the serial code. We measure the threading costs and discuss how a future computer architecture will enable this optimisation to be feasible in exploiting thread instead of instruction and/or vector parallelism.},
author = {Rogers, I. and Kirkham, C. and Watson, I.},
booktitle = {Sixth International Conference on Parallel and Distributed Computing Applications and Technologies (PDCAT'05)},
doi = {10.1109/PDCAT.2005.164},
isbn = {0-7695-2405-2},
pages = {35--39},
publisher = {IEEE},
title = {{Loop Parallelisation for the Jikes RVM}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1578860},
year = {2005}
}
@article{Nayfeh1997,
abstract = {Presents the case for billion-transistor processor architectures that will consist of chip multiprocessors (CMPs): multiple (four to 16) simple, fast processors on one chip. In their proposal, each processor is tightly coupled to a small, fast, level-one cache, and all processors share a larger level-two cache. The processors may collaborate on a parallel job or run independent tasks (as in the SMT proposal). The CMP architecture lends itself to simpler design, faster validation, cleaner functional partitioning, and higher theoretical peak performance. However for this architecture to realize its performance potential, either programmers or compilers will have to make code explicitly parallel. Old ISAs will be incompatible with this architecture (although they could run slowly on one of the small processors)},
author = {Nayfeh, B.A. and Olukotun, K.},
doi = {10.1109/2.612253},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Nayfeh, Olukotun - 1997 - A single-chip multiprocessor.html:html},
issn = {00189162},
journal = {Computer},
number = {9},
pages = {79--85},
title = {{A single-chip multiprocessor}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=612253},
volume = {30},
year = {1997}
}
@article{Carter1979,
author = {Carter, J.Lawrence and Wegman, Mark N},
doi = {10.1016/0022-0000(79)90044-8},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
month = apr,
number = {2},
pages = {143--154},
title = {{Universal classes of hash functions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0022000079900448},
volume = {18},
year = {1979}
}
@article{Dubach2012,
author = {Dubach, Christophe and Cheng, Perry and Rabbah, Rodric and Bacon, David F. and Fink, Stephen J.},
doi = {10.1145/2345156.2254066},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Dubach et al. - 2012 - Compiling a high-level language for GPUs via language support for architectures and compilers.pdf:pdf},
isbn = {978-1-4503-1205-9},
issn = {0362-1340},
journal = {ACM SIGPLAN Notices},
keywords = {gpu,java,lime,map,opencl,reduce,streaming},
month = aug,
number = {6},
pages = {1--1--12--12},
publisher = {ACM},
title = {{Compiling a high-level language for GPUs via language support for architectures and compilers}},
url = {http://dl.acm.org/citation.cfm?id=2345156.2254066},
volume = {47},
year = {2012}
}
@book{Cole,
abstract = {In the past, most significant improvements in computer performance have been achieved as a result of advances in simple device technology. The introduction of large scale parallelism at the inter-processor level now represents a viable alter- native. However, this method also introduces new difficulties, most notably the conceptual barrier encountered by the user of such a system in efficiently coor- dinating many concurrent activities towards a single goal. Thus, the design and implementation of software systems which can ease this burden is of increasing importance. Such a system must find a good balance between the simplicity of the interface presented and the efficiency with which it can be implemented. This book considers existing work in the area and proposes a new approach. The new system presents the user with a selection of independent “algorithmic skeletons”, each of which describes the structure of a particular style of algorithm, in the way in which “higher order functions” represent general computational frameworks in the context of functional programming languages. The user must describe a solution to a problem as an instance of the appropriate skeleton. The implementation task is simplified by the fact that each skeleton may be considered independently, in contrast to the monolithic programming interfaces of existing systems at a similar level of abstraction. The four skeletons presented here are based on the notions of “fixed degree di- vide and conquer”, “iterative combination” “clustering” and “task queues”. Each skeleton is introduced in terms of the abstraction it presents to the user. Imple- mentation on a square grid of autonomous processor-memory pairs is considered, and examples of problems which could be solved in terms of the skeleton are presented. In conclusion, the strengths and weaknesses of the “skeletal” approach are assessed in the context of the existing alternatives.},
address = {Edinburgh},
author = {Cole, Murray},
edition = {First},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Cole - 1989 - Algorithmic Skeletons Structured Management of Parallel Computation Table of Contents.pdf:pdf},
publisher = {MIT Press},
title = {{Algorithmic Skeletons : Structured Management of Parallel Computation Table of Contents}},
url = {http://homepages.inf.ed.ac.uk/mic/Pubs/skeletonbook.pdf},
year = {1989}
}
@inproceedings{Patel,
abstract = {For a variety of reasons, branch-less regions of instructions are desirable for high-performance execution. In this paper we propose a means for increasing the dynamic length of branch-less regions of instructions for the purposes of dynamic program optimization. We call these atomic regions frames and we construct them by replacing original branch instructions with assertions. Assertion instructions check if the original branching conditions still hold. If they hold, no action is taken. If they do not, then the entire region is undone. In this manner an assertion has no explicit control flow. We demonstrate that using branch correlation to decide when a branch should be converted into an assertion results in atomic regions that average over 100 instructions in length, with a probability of completion of 97\%, and that constitute over 80\% of the dynamic instruction stream. We demonstrate both static and dynamic means for constructing frames. When frames are built dynamically using finite sized hardware, they average 80 instructions in length and have good caching properties},
author = {Patel, S.J. and Tung, T. and Bose, S. and Crum, M.M.},
booktitle = {Proceedings 33rd Annual IEEE/ACM International Symposium on Microarchitecture. MICRO-33 2000},
doi = {10.1109/MICRO.2000.898080},
isbn = {0-7695-0924-X},
pages = {303--313},
publisher = {IEEE},
title = {{Increasing the size of atomic instruction blocks using control flow assertions}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=898080}
}
@inproceedings{Herlihy1993,
abstract = {A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock.},
author = {Herlihy, M and Eliot, J and Moss, B},
booktitle = {Proceedings of the 20th Annual International Symposium on Computer Architecture},
doi = {10.1109/ISCA.1993.698569},
isbn = {0-8186-3810-9},
keywords = {memory},
mendeley-tags = {memory},
pages = {289--300},
publisher = {IEEE Comput. Soc. Press},
title = {{Transactional Memory: Architectural Support For Lock-free Data Structures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=698569},
volume = {21},
year = {1993}
}
@article{Park2005,
author = {Park, Laurence A. F. and Ramamohanarao, Kotagiri and Palaniswami, Marimuthu},
doi = {10.1145/1080343.1080345},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Park, Ramamohanarao, Palaniswami - 2005 - A novel document retrieval method using the discrete wavelet transform.pdf:pdf},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
keywords = {Daubechies,Haar,document retrieval,multiresolution analysis,proximity search,vector space methods,wavelet transform},
month = jul,
number = {3},
pages = {267--298},
publisher = {ACM},
title = {{A novel document retrieval method using the discrete wavelet transform}},
url = {http://dl.acm.org/citation.cfm?id=1080343.1080345},
volume = {23},
year = {2005}
}
@book{Azar2006,
address = {Berlin, Heidelberg},
doi = {10.1007/11841036},
editor = {Azar, Yossi and Erlebach, Thomas},
isbn = {978-3-540-38875-3},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Algorithms – ESA 2006}},
url = {http://www.springerlink.com/index/10.1007/11841036},
volume = {4168},
year = {2006}
}
@article{Gamma1995,
abstract = {Capturing a wealth of experience about the design of object-oriented software, four top-notch designers present a catalog of simple and succinct solutions to commonly occurring design problems. Previously undocumented, these 23 patterns allow designers to create more flexible, elegant, and ultimately reusable designs without having to rediscover the design solutions themselves. The authors begin by describing what patterns are and how they can help you design object-oriented software. They then go on to systematically name, explain, evaluate, and catalog recurring designs in object-oriented systems. With Design Patterns as your guide, you will learn how these important patterns fit into the software development process, and how you can leverage them to solve your own design problems most efficiently. Each pattern describes the circumstances in which it is applicable, when it can be applied in view of other design constraints, and the consequences and trade-offs of using the pattern within a larger design. All patterns are compiled from real systems and are based on real-world examples. Each pattern also includes code that demonstrates how it may be implemented in object-oriented programming languages like C++ or Smalltalk. 0201633612B07092001},
author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
doi = {10.1093/carcin/bgs084},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Gamma et al. - 1995 - Design Patterns Elements of Reusable Object-Oriented Software.pdf:pdf},
isbn = {0201633612},
issn = {0143-3334},
journal = {Carcinogenesis},
keywords = {computacao (metodologia e tecnicas) |2 larpcal,gang of four,gof,pattern},
mendeley-tags = {computacao (metodologia e tecnicas) |2 larpcal,gang of four,gof,pattern},
month = aug,
number = {8},
pages = {NP--NP},
pmid = {22693012},
publisher = {Addison-Wesley},
title = {{Table\_of\_Contents}},
url = {http://www.carcin.oxfordjournals.org/cgi/doi/10.1093/carcin/bgs084},
volume = {33},
year = {2012}
}
@article{Mandelbrot1980,
abstract = {L and z-fractals, mandelbrot set, z-fractal as fractal attractor, strange fractal attractors},
author = {Mandelbrot, B B},
journal = {Ann N Y Ac Sci},
pages = {249},
title = {{Fractal aspects of the iteration of z->l z (1-z) for complex l and z}},
volume = {357},
year = {1980}
}
@article{Kiczales1997,
abstract = {This position statement presents the concept of Aspect-Oriented Programming as a promising area of research in programming languages and software engineering.},
author = {Kiczales, Gregor},
doi = {10.1145/242224.242420},
isbn = {3540630899},
issn = {03600300},
journal = {ACM Computing Surveys},
month = dec,
number = {4es},
pages = {154--es},
pmid = {11346780},
title = {{Aspect-oriented programming}},
url = {http://portal.acm.org/citation.cfm?doid=242224.242420},
volume = {28},
year = {1996}
}
@misc{Pthreads,
author = {{Institute for Electrical and Electronic Engineers} and {The Open Group}},
title = {{POSIX Specifications, Section 1.c: Thread Extensions}},
url = {http://pubs.opengroup.org/onlinepubs/9699919799/functions/V2\_chap02.html\#tag\_15\_09},
urldate = {1/09/2013},
year = {2001}
}
@article{Nyland2007,
abstract = {An N-body simulation numerically approximates the evolution of a system of bodies in which each body continuously interacts with every other body. A familiar example is an astrophysical simulation in which each body represents a galaxy or an individual star, and the bodies attract each other through the gravitational force, as in Figure 31-1. N-body},
author = {Nyland, Lars and Prins, Jan},
isbn = {0321515269},
journal = {Simulation},
keywords = {gpgpu,n-body},
mendeley-tags = {gpgpu,n-body},
pages = {677--696},
title = {{Fast N-Body Simulation with CUDA}},
volume = {3},
year = {2007}
}
@article{Weiser,
author = {Weiser, M.},
issn = {0020-0190},
journal = {Information processing letters},
keywords = {Parallel processing,Programmation,Programming,Traitement parall\`{e}le},
language = {eng},
number = {3},
pages = {129--135},
publisher = {Elsevier},
title = {{Reconstructing sequential behavior from parallel behavior projections}},
url = {http://cat.inist.fr/?aModele=afficheN\&cpsidt=9606727},
volume = {17}
}
@inproceedings{Sanchez2007,
abstract = {Transactional Memory (TM) systems must track the read and write sets - items read and written during a transaction - to detect conflicts among concurrent transactions. Several TMs use signatures, which summarize unbounded read/write sets in bounded hardware at a performance cost of false positives (conflicts detected when none exists). This paper examines different organizations to achieve hardware-efficient and accurate TM signatures. First, we find that implementing each signature with a single k-hash- function Bloom filter (True Bloom signature) is inefficient, as it requires multi-ported SRAMs. Instead, we advocate using k single-hash-function Bloom filters in parallel (Parallel Bloom signature), using area-efficient single-ported SRAMs. Our formal analysis shows that both organizations perform equally well in theory and our simulation- based evaluation shows this to hold approximately in practice. We also show that by choosing high-quality hash functions we can achieve signature designs noticeably more accurate than the previously proposed implementations. Finally, we adapt Pagh and Rodler's cuckoo hashing to implement Cuckoo-Bloom signatures. While this representation does not support set intersection, it mitigates false positives for the common case of small read/write sets and performs like a Bloom filter for large sets.},
author = {Sanchez, Daniel and Yen, Luke and Hill, Mark D. and Sankaralingam, Karthikeyan},
booktitle = {40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007)},
doi = {10.1109/MICRO.2007.24},
isbn = {0-7695-3047-8},
pages = {123--133},
publisher = {IEEE},
title = {{Implementing Signatures for Transactional Memory}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4408250},
year = {2007}
}
@misc{RoslynProject,
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Roslyn Project.html:html},
keywords = {C\#,CSharp,CTP,Roslyn,VB,Visual Basic},
title = {{Roslyn Project}},
url = {https://www.microsoft.com/en-us/download/details.aspx?id=27744},
urldate = {11/04/13}
}
@misc{BloomFilterWikiDiagram,
author = {Wikipedia},
booktitle = {Wikipedia},
title = {{Bloom filter diagram}},
url = {https://en.wikipedia.org/wiki/File:Bloom\_filter.svg},
urldate = {4/09/2013},
year = {2013}
}
@misc{LoopPrediction,
author = {Peng, Chang},
booktitle = {OpenJDK Wiki},
title = {{Loop Prediction}},
url = {https://wikis.oracle.com/display/HotSpotInternals/LoopPredication},
urldate = {02/08/2013},
year = {2010}
}
@article{Dong,
author = {Dong, Guoxing and Chen, Kai and Zhu, Erzhou and Zhang, Yichao and Qi, Zhengwei and Guan, Haibing},
doi = {10.1109/PAAP.2010.53},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - Unknown - A Translation Framework for Virtual Execution Environment on CPU GPU Architecture.pdf:pdf},
isbn = {9780769543123},
keywords = {-gpgpu,cuda,parallelization,translator},
title = {{A Translation Framework for Virtual Execution Environment on CPU / GPU Architecture}}
}
@misc{GuavaBloomFilter,
author = {Google, Inc},
booktitle = {Guava Documentation},
title = {{Class BloomFilter<T>}},
url = {http://docs.guava-libraries.googlecode.com/git-history/release/javadoc/com/google/common/hash/BloomFilter.html},
urldate = {2/08/2014},
year = {2013}
}
@book{Furia2012,
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-642-30561-0},
editor = {Furia, Carlo A. and Nanz, Sebastian},
isbn = {978-3-642-30560-3},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Objects, Models, Components, Patterns}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-30561-0},
volume = {7304},
year = {2012}
}
@article{Ogata2010,
author = {Ogata, Kazunori and Mikurube, Dai and Kawachiya, Kiyokuni and Trent, Scott and Onodera, Tamiya},
doi = {10.1145/1932682.1869477},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Ogata et al. - 2010 - A study of Java's non-Java memory.pdf:pdf},
isbn = {978-1-4503-0203-6},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {java,java native memory,memory footprint analysis,non-java memory},
month = oct,
number = {10},
pages = {191},
publisher = {ACM},
title = {{A study of Java's non-Java memory}},
url = {http://dl.acm.org/citation.cfm?id=1932682.1869477},
volume = {45},
year = {2010}
}
@incollection{Magnusson2002,
address = {Berlin, Heidelberg},
author = {Glew, Neal and Palsberg, Jen},
booktitle = {ECOOP 2002 — Object-Oriented Programming},
doi = {10.1007/3-540-47993-7},
editor = {Magnusson, Boris},
isbn = {978-3-540-43759-8},
month = may,
pages = {525--544},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Type-Safe Method Inlining}},
url = {http://link.springer.com/10.1007/3-540-47993-7},
volume = {2374},
year = {2002}
}
@book{DSandAlgsCpp,
author = {Sartaj, Sahni},
edition = {first},
isbn = {0-07-109219-6},
publisher = {McGraw-Hill},
title = {{Data Structures, Algorithms and Applications in C++}},
year = {1998}
}
@inproceedings{Warren1993,
abstract = {VERY GOOD PAPER Barnes-Hut higher accuracy algorithm. Introduction of the HASHED octree idea Discussion on tree construction an Pealno-Hilbert, Morton order ordering Discussion on the special data-structures. Tree construction less than 7\% of the computation. 4.9 GFlops/s},
author = {Warren, Michael S and Salmon, John K},
booktitle = {Proceedings of Supercomputing},
title = {{A parallel Hashed Oct-Tree N-Body Algorithm}},
year = {1993}
}
@misc{ppls,
address = {Edinburgh},
author = {Cole, Murray},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Cole - 2013 - Parallel Programming Languages and Systems.pdf:pdf},
publisher = {University of Edinburgh},
title = {{Parallel Programming Languages and Systems}},
url = {http://www.inf.ed.ac.uk/teaching/courses/ppls/pplsslides.pdf},
year = {2013}
}
@article{Stone1991,
abstract = {Some of the technology that will drive the advances of the 1990s are explored. A brief tutorial is given to explain the fundamental speed limits of metal interconnections. The advantages and disadvantages of optical interconnections and where they may be used are discussed in some detail. Trends in speeding up performance by increasing data-path width and by increasing the number of operations performed are reviewed, and questions of efficiency are examined. The advent of super reliable machines produced at very low cost by replicating entire processors is examined.},
author = {Stone, H.S. and Cocke, J.},
doi = {10.1109/2.84897},
issn = {0018-9162},
journal = {Computer},
month = sep,
number = {9},
pages = {30--38},
title = {{Computer architecture in the 1990s}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=84897},
volume = {24},
year = {1991}
}
@unpublished{Mainland2013,
author = {Mainland, Geoffrey and Leshchinskiy, Roman and Peyton-Jones, Simon},
booktitle = {ACM SIGPLAN International Conference on Functional Programming},
doi = {2500365.2500601},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Mainland, Leshchinskiy, Peyton-Jones - 2013 - Exploiting Vector Instructions with Generalized Stream Fusion.pdf:pdf},
isbn = {9781450323260},
keywords = {12,15,23,29,30,34,8,allow some of these,ate structures to be,eliminated,f,finding more general rules,g,great deal of research,has been,haskell,intermedi-,map,map g,simd,stream fusion,such as map f,the subject of a,vectorization},
title = {{Exploiting Vector Instructions with Generalized Stream Fusion}},
url = {http://research.microsoft.com/en-us/um/people/simonpj/papers/ndp/haskell-beats-C.pdf},
year = {2013}
}
@article{Dijkstra1968,
abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
author = {Dijkstra, Edsger W},
doi = {10.1145/362929.362947},
issn = {00010782},
journal = {Communications of the ACM},
month = mar,
number = {3},
pages = {147--148},
title = {{Letters to the editor: go to statement considered harmful}},
url = {http://portal.acm.org/citation.cfm?doid=362929.362947},
volume = {11},
year = {1968}
}
@article{Swamidass2007,
abstract = {In many modern chemoinformatics systems, molecules are represented by long binary fingerprint vectors recording the presence or absence of particular features or substructures, such as labeled paths or trees, in the molecular graphs. These long fingerprints are often compressed to much shorter fingerprints using a simple modulo operation. As the length of the fingerprints decreases, their typical density and overlap tend to increase, and so does any similarity measure based on overlap, such as the widely used Tanimoto similarity. Here we show that this correlation between shorter fingerprints and higher similarity can be thought of as a systematic error introduced by the fingerprint folding algorithm and that this systematic error can be corrected mathematically. More precisely, given two molecules and their compressed fingerprints of a given length, we show how a better estimate of their uncompressed overlap, hence of their similarity, can be derived to correct for this bias. We show how the correction can be implemented not only for the Tanimoto measure but also for all other commonly used measures. Experiments on various data sets and fingerprint sizes demonstrate how, with a negligible computational overhead, the correction noticeably improves the sensitivity and specificity of chemical retrieval.},
author = {Swamidass, S Joshua and Baldi, Pierre},
doi = {10.1021/ci600526a},
issn = {1549-9596},
journal = {Journal of chemical information and modeling},
keywords = {Chemistry,Chemistry: methods,Databases,Factual,Informatics,Informatics: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Mathematics},
number = {3},
pages = {952--64},
pmid = {17444629},
title = {{Mathematical correction for fingerprint similarity measures to improve chemical retrieval.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17444629},
volume = {47},
year = {2007}
}
@misc{javaagents,
author = {Javabeats.com},
title = {{Introduction to Java Agents}},
url = {http://www.javabeat.net/2012/06/introduction-to-java-agents/},
urldate = {28/07/2013},
year = {2012}
}
@article{Trenti2008,
abstract = {Gravitational N-body simulations, that is numerical solutions of the equations of motions for N particles interacting gravitationally, are widely used tools in astrophysics, with applications from few body or solar system like systems all the way up to galactic and cosmological scales. In this article we present a summary review of the field highlighting the main methods for N-body simulations and the astrophysical context in which they are usually applied.},
archivePrefix = {arXiv},
arxivId = {0806.3950},
author = {Trenti, M and Hut, P},
eprint = {0806.3950},
file = {:afs/inf.ed.ac.uk/user/s12/s1255753/Documents/\_mendeley/Trenti, Hut/Scholarpedia/Trenti, Hut - 2008 - Gravitational N-body Simulations - Scholarpedia.pdf:pdf},
journal = {Scholarpedia},
pages = {13},
title = {{Gravitational N-body Simulations}},
volume = {3},
year = {2008}
}
@article{Nvidia2011,
abstract = {With this framework, programmers can easily write GPGPU programs without any ... The first is CUDA's affinity for C: it is easy to translate C into CUDA ...},
author = {Nvidia, C},
isbn = {9783642106712},
journal = {Changes},
pages = {173},
title = {{NVIDIA CUDA C Programming Guide}},
year = {2011}
}
@inproceedings{Hofstee,
abstract = {This paper provides a background and rationale for some of the architecture and design decisions in the cell processor, a processor optimized for compute-intensive and broadband rich media applications, jointly developed by Sony Group, Toshiba, and IBM. The paper discusses some of the challenges microprocessor designers face and provides motivation for performance per transistor as a reasonable first-order metric for design efficiency. Common microarchitectural enhancements relative to this metric are provided. Also alternate architectural choices and some of its limitations are discussed and non-homogeneous SMP as a means to overcome these limitations is proposed.},
author = {Hofstee, H.P.},
booktitle = {11th International Symposium on High-Performance Computer Architecture},
doi = {10.1109/HPCA.2005.26},
isbn = {0-7695-2275-0},
pages = {258--262},
publisher = {IEEE},
title = {{Power Efficient Processor Architecture and The Cell Processor}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1385948}
}
@article{Grossman2011,
abstract = {The computer industry is at a major inflection point in its hardware roadmap due to the end of a decades-long trend of exponentially increasing clock frequencies. Instead, future computer systems are expected to be built using homogeneous and heterogeneous many-core processors with 10's to 100's of cores per chip, and complex hardware designs to address the challenges of concurrency, energy efficiency and resiliency. Unlike previous generations of hardware evolution, this shift towards many-core computing will have a profound impact on software. These software challenges are further compounded by the need to enable parallelism in workloads and application domains that traditionally did not have to worry about multiprocessor parallelism in the past. A recent trend in mainstream desktop systems is the use of graphics processor units (GPUs) to obtain order-of-magnitude performance improvements relative to general-purpose CPUs. Unfortunately, hybrid programming models that support multithreaded execution on CPUs in parallel with CUDA execution on GPUs prove to be too complex for use by mainstream programmers and domain experts, especially when targeting platforms with multiple CPU cores and multiple GPU devices. In this paper, we extend past work on Intel's Concurrent Collections (CnC) programming model to address the hybrid programming challenge using a model called CnC-CUDA. CnC is a declarative and implicitly parallel coordination language that supports flexible combinations of task and data parallelism while retaining determinism. CnC computations are built using steps that are related by data and control dependence edges, which are represented by a CnC graph. The CnC-CUDA extensions in this paper include the definition of multithreaded steps for execution on GPUs, and automatic generation of data and control flow between CPU steps and GPU steps. Experimental results show that this approach can yield significant performance benefits with both GPU execution and hybrid CPU/GPU execution. 2011 Springer-Verlag Berlin Heidelberg.},
author = {Grossman, M and {Simion Sb\^{\i}rlea}, A and Budimli\'{c}, Z and Sarkar, V},
journal = {Lecture Notes in Computer Science including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics},
pages = {230--245},
title = {{CnC-CUDA: Declarative programming for GPUs}},
volume = {6548 LNCS},
year = {2011}
}
@article{Callahan1987,
author = {Callahan, C.D. II},
keywords = {algorithms,and information science,array processors,computers,computing,general and miscellaneous//mathematics,mathematical logic,optimization,parallel processing,programming,vector processing},
language = {English},
month = jan,
publisher = {Rice Univ.,Houston, TX},
title = {{Global approach to detection of parallelism}},
url = {http://www.osti.gov/energycitations/product.biblio.jsp?osti\_id=7055331},
year = {1987}
}
@inproceedings{Sun2011,
abstract = {We propose a new approach that automatically parallelizes Java programs at runtime. The approach collects on-line trace information during program execution, and dynamically recompiles methods that can be executed in parallel. Wealso describe a cost/benefit model that makes intelligent parallelization decisions, as well as a parallel execution environment to execute parallelized code. We implement these techniques upon Jikes RVM and evaluate our approach by parallelizing sequential benchmarks and comparing the performance to manually parallelized version of those benchmarks. According to the experimental results, our approach has low overheads and achieves competitive speed-ups compared to manually parallelized code.},
author = {Sun, Yu and Zhang, Wei},
booktitle = {2011 15th Workshop on Interaction between Compilers and Computer Architectures},
doi = {10.1109/INTERACT.2011.11},
isbn = {978-1-4577-0834-3},
month = feb,
pages = {35--43},
publisher = {IEEE},
title = {{On-Line Trace Based Automatic Parallelization of Java Programs on Multicore Platforms}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5936720},
year = {2011}
}
@article{MERRILL2011,
abstract = {The need to rank and order data is pervasive, and many algorithms are fundamentally dependent upon sorting and partitioning operations. Prior to this work, GPU stream processors have been perceived as challenging targets for problems with dynamic and global data-dependences such as sorting. This paper presents: (1) a family of very efficient parallel algorithms for radix sorting; and (2) our allocation-oriented algorithmic design strategies that match the strengths of GPU processor architecture to this genre of dynamic parallelism. We demonstrate multiple factors of speedup (up to 3.8x) compared to state-of-the-art GPU sorting. We also reverse the performance differentials observed between GPU and multi/many-core CPU architectures by recent comparisons in the literature, including those with 32-core CPU-based accelerators. Our average sorting rates exceed 1B 32-bit keys/sec on a single GPU microprocessor. Our sorting passes are constructed from a very efficient parallel prefix scan "runtime" that incorpor...},
author = {MERRILL, DUANE and GRIMSHAW, ANDREW},
doi = {10.1142/S0129626411000187},
issn = {0129-6264},
journal = {Parallel Processing Letters},
keywords = {GPU,Parallel sorting,kernel fusion,prefix scan,prefix sum,radix sorting},
language = {en},
month = jun,
number = {02},
pages = {245--272},
publisher = {World Scientific Publishing Company},
title = {{HIGH PERFORMANCE AND SCALABLE RADIX SORTING: A CASE STUDY OF IMPLEMENTING DYNAMIC PARALLELISM FOR GPU COMPUTING}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0129626411000187},
volume = {21},
year = {2011}
}
@article{Fox1999,
abstract = {We describe the definition, motivation and current status of Java Grande activities. We introduce 3 roles of Java in Grande programming at client, middleware or backend tiers of a computing system. We start with Java as a language and describe where it is clearly good and where it could be good! The Java Grande Forum has numerical and distributed computing working groups and projects include the study of changes to Java and its runtime to enhance Grande applications and their programming environment community. There is an important activity to define seamless interfaces allowing universal access to general hosts. Benchmarks for all sorts of Grande applications are critical. We discuss Java for Parallel Computing including message passing (MPI) and data parallelism},
author = {Fox, G},
doi = {10.1109/FMPC.1999.750606},
file = {:afs/inf.ed.ac.uk/user/s12/s1255753/Documents/\_mendeley/Fox/Proceedings Frontiers 99 Seventh Symposium on the Frontiers of Massively Parallel Computation/Fox - 1999 - Java Grande software infrastructure for HPCC - Proceedings Frontiers 99 Seventh Symposium on the Frontiers of Massively Par.pdf:pdf},
isbn = {0769500870},
journal = {Proceedings Frontiers 99 Seventh Symposium on the Frontiers of Massively Parallel Computation},
title = {{Java Grande: software infrastructure for HPCC}},
year = {1999}
}
@article{Holzle1992,
abstract = {SELF's debugging system provides complete source-level debugging (expected behavior) with globally optimized code. It shields the debugger from optimizations performed by the compiler by dynamically code on demand. Deoptimization only affects the procedure activations that are actively being debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging information at discrete interrupt points; the compiler can still perform extensive optimizations between interrupt points without affecting debuggability. At the same time, the inability to interrupt between interrupt points is invisible to the user. Our debugging system also handles programming changes during debugging. Again, the system provides expected behavior: it is possible to change a running program and immediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which may contain inlined copies of the old version of the changed procedure) into new versions reflecting the current source-level state. To the best of our knowledge, SELF is the first practical system providing full expected behavior with globally optimized code.},
author = {H\"{o}lzle, Urs and Chambers, Craig and Ungar, David},
doi = {10.1145/143103.143114},
isbn = {0897914759},
issn = {03621340},
journal = {ACM Sigplan Notices},
pages = {32--43},
title = {{Debugging optimized code with dynamic deoptimization}},
volume = {27},
year = {1992}
}
@article{Bloom1970,
author = {Bloom, Burton H.},
doi = {10.1145/362686.362692},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Bloom - 1970 - Spacetime trade-offs in hash coding with allowable errors.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {hash addressing,hash coding,retrieval efficiency,retrieval trade-offs,scatter storage,searching,storage efficiency,storage layout},
month = jul,
number = {7},
pages = {422--426},
publisher = {ACM},
title = {{Space/time trade-offs in hash coding with allowable errors}},
url = {http://dl.acm.org/citation.cfm?id=362686.362692},
volume = {13},
year = {1970}
}
@misc{HashTableWikiDiagram,
author = {Wikipedia},
booktitle = {Wikipedia},
title = {{Hash table diagram}},
url = {https://en.wikipedia.org/wiki/File:Hash\_table\_3\_1\_1\_0\_1\_0\_0\_SP.svg},
urldate = {4/08/2013},
year = {2013}
}
@misc{Graal,
author = {Oracle and OpenJDK},
title = {{Graal Project}},
url = {http://openjdk.java.net/projects/graal/},
urldate = {26/07/2013},
year = {2012}
}
@techreport{Fraser04,
address = {Cambridge},
author = {Fraser, Keir},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Fraser - 2004 - Technical Report.pdf:pdf},
institution = {University of Cambridge},
number = {579},
title = {{Technical Report}},
url = {http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf},
year = {2004}
}
@inproceedings{Bull2001,
abstract = {Increasing interest is being shown in the use of Java for scientific applications. The Java Grande benchmark suite 4 was designed with such applications primarily in mind. The perceived lack of performance of Java still deters many potential users, despite recent advances in just-in-time (JIT) and adaptive compilers. There are however few benchmark results available comparing Java to more traditional languages such as C and Fortran. To address this issue, a subset of the Java Grande Benchmarks have been re-written in C and Fortran allowing direct performance comparisons between the three languages. The performance of a range of Java execution environments, C and Fortran compilers have been tested across a number of platforms using the suite. These demonstrate that on some platforms (notably Intel Pentium) the performance gap is now quite small.},
address = {New York, New York, USA},
author = {Bull, J M and Smith, L A and Pottage, L and Freeman, R},
booktitle = {Proceedings of the 2001 joint ACM-ISCOPE conference on Java Grande - JGI '01},
doi = {10.1145/376656.376823},
isbn = {1581133596},
issn = {15320626},
pages = {97--105},
publisher = {ACM Press},
title = {{Benchmarking Java against C and Fortran for scientific applications}},
url = {http://portal.acm.org/citation.cfm?doid=376656.376823},
volume = {15},
year = {2001}
}
@misc{boxing,
author = {Oracle},
booktitle = {Java Language Guide},
title = {{Autoboxing}},
url = {http://docs.oracle.com/javase/1.5.0/docs/guide/language/autoboxing.html},
urldate = {4/08/2013},
year = {2010}
}
@techreport{McDonald2008,
author = {McDonald, Jason},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/McDonald - 2008 - Design Patterns.pdf:pdf},
institution = {DZone},
title = {{Design Patterns}},
url = {http://cs.franklin.edu/~whittakt/COMP311/rc008-designpatterns\_online.pdf},
year = {2008}
}
@inproceedings{Mansour1990,
address = {New York, New York, USA},
author = {Mansour, Y. and Nisan, N. and Tiwari, P.},
booktitle = {Proceedings of the twenty-second annual ACM symposium on Theory of computing - STOC '90},
doi = {10.1145/100216.100246},
file = {:afs/inf.ed.ac.uk/user/s12/s1255753/Documents/\_mendeley/Mansour, Nisan, Tiwari/Proceedings of the twenty-second annual ACM symposium on Theory of computing - STOC '90/Mansour, Nisan, Tiwari - 1990 - The computational complexity of universal hashing - Proceedings of the twenty-second annual ACM symposiu.pdf:pdf},
isbn = {0897913612},
month = apr,
pages = {235--243},
publisher = {ACM Press},
title = {{The computational complexity of universal hashing}},
url = {http://dl.acm.org/citation.cfm?id=100216.100246},
year = {1990}
}
@article{Kim2000,
author = {Kim, Jin-Soo and Hsu, Yarsun},
doi = {10.1145/345063.339422},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Hsu - 2000 - Memory system behavior of Java programs.pdf:pdf},
isbn = {1-58113-194-1},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
month = jun,
number = {1},
pages = {264--274},
publisher = {ACM},
title = {{Memory system behavior of Java programs}},
url = {http://dl.acm.org/citation.cfm?id=345063.339422},
volume = {28},
year = {2000}
}
@incollection{ACDI,
address = {San Francisco},
author = {Muchnick, Steven S.},
edition = {first},
isbn = {1-55860-320-4},
publisher = {Morgan Kaufmann},
title = {{Advanced Compiler Design and Implementation}},
year = {1997}
}
@article{Chen2007,
abstract = {Join points and advices are two fundamental constructs of aspect-oriented programming languages. AspectJ provides a large set of useful pointcuts that enables aspect-oriented programmers to pick out target join points for advice weaving in a highly flexible manner. However, the field access pointcuts of AspectJ do not support array objects in full. When an element of an array field is set or referenced, the corresponding index values and assigned value are not exposed to the advice. This paper presents an extension of AspectJ’s field access pointcuts to arrays that exposes such useful context information. We have implemented this extension using the abc compiler for AspectJ. The core of our implementation is a finite-state machine based pointcut matcher that can handle arrays of multiple dimensions in a uniform way.},
author = {Chen, Kung and Chien, Chin-hung},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Chien - 2007 - Extending the Field Access Pointcuts of AspectJ to Arrays.pdf:pdf},
journal = {Journal of Software Engineering Studies},
number = {2},
pages = {2--11},
title = {{Extending the Field Access Pointcuts of AspectJ to Arrays}},
url = {http://www.geocities.ws/m8809301/pub/JSESv2n2\_KungChen\_970214.pdf},
volume = {2},
year = {2007}
}
@article{Blackburn2006,
abstract = {Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex run-time tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages. Copyright \&copy; 2006 ACM.},
author = {Blackburn, S M and Garner, R and Hoffman, C and Khan, A M and McKinley, K S and Bentzur, R and Diwan, A and Feinberg, D and Frampton, D and Guyer, S Z and Hirzel, M and Hosking, A and Jump, M and Lee, H and Moss, J E B and Phansalkar, A and Stefanovi\'{c}, D and VanDrunen, T and {Von Dincklage}, D and Wiedermann, B},
doi = {10.1145/1167515.1167488},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Blackburn et al. - 2006 - The DaCapo benchmarks Java benchmarking development and analysis.pdf:pdf},
isbn = {1595933484},
issn = {03621340},
journal = {ACM Sigplan Notices},
keywords = {benchmark,dacapo,java,methodology,spec},
mendeley-tags = {benchmark,java},
pages = {169--190},
title = {{The DaCapo benchmarks: Java benchmarking development and analysis}},
volume = {41},
year = {2006}
}
@inproceedings{Chiba1998,
abstract = {This paper presents the Javassist system, which is a programming tool for assisting Java programmers. It enables programmers to write a meta-level program automating some kinds of class definitions. Moreover, a number of applications of runtime reflection can be implemented with this system.},
author = {Chiba, Shigeru},
booktitle = {Proceedings of the OOPSLA workshop on Reflective Programming in C and},
title = {{Javassist - A Reflection-Based Programming Wizard for Java}},
url = {http://www.csg.ci.i.u-tokyo.ac.jp/~chiba/oopsla98/proc/chiba.pdf},
year = {1998}
}
@inproceedings{PeytonJones2011,
address = {Melbourne},
author = {Peyton-Jones, Simon},
booktitle = {YOW},
publisher = {Presented at YOW 2011},
title = {{The Future is Parallel, and the Future of Parallel is Declarative}},
url = {http://yow.eventer.com/yow-2011-1004/the-future-is-parallel-and-the-future-of-parallel-is-declarative-by-simon-peyton-jones-1055},
year = {2011}
}
@techreport{JVMSpec,
address = {Redwood City},
author = {Lindholm, Tim and Buckley, Alex and Bracha, Gilad and Yellin, Frank},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Lindholm et al. - 2013 - The Java Virtual Machine Specification Java SE 7 Edition.pdf:pdf},
institution = {Oracle, Inc},
title = {{The Java Virtual Machine Specification Java SE 7 Edition}},
url = {http://docs.oracle.com/javase/specs/jvms/se7/jvms7.pdf},
year = {2013}
}
@article{Ghemawat2003,
author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
doi = {10.1145/1165389.945450},
isbn = {1-58113-757-5},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
keywords = {clustered storage,data storage,fault tolerance,scalability},
month = dec,
number = {5},
pages = {29},
publisher = {ACM},
title = {{The Google file system}},
url = {http://dl.acm.org/citation.cfm?id=1165389.945450},
volume = {37},
year = {2003}
}
@misc{JavassistDocs,
author = {{Javassist Project}},
title = {{Javasssist documentation}},
urldate = {31/07/2013},
year = {2013}
}
@article{Smith2001,
abstract = {Increasing interest is being shown in the use of Java for large scale or Grande applications. This new use of Java places speci.c demands on the Java execution environments that can be tested using the Java Grande benchmark suite 5, 6, 7. The large processing requirements of Grande applications makes parallelisation of interest. A suite of parallel benchmarks has been developed from the serial Java Grande benchmark suite, using three parallel programming models: Java native threads, MPJ (a message passing interface) and JOMP (a set of OpenMP-like directives). The contents of the suite are described, and results presented for a number of platforms.},
author = {Smith, L A and Bull, J M and Obdrizalek, J},
doi = {10.1109/SC.2001.10025},
isbn = {158113293X},
journal = {ACMIEEE SC 2001 Conference SC01},
keywords = {benchmarking,java,message passing,parallel,threads.},
pages = {8--8},
title = {{A Parallel Java Grande Benchmark Suite}},
year = {2001}
}
@article{Yang2011,
author = {Yang, Jing and Skadron, Kevin and Soffa, Mary Lou and Whitehouse, Kamin},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2011 - Feasibility of Dynamic Binary Parallelization.pdf:pdf},
journal = {Usenix},
title = {{Feasibility of Dynamic Binary Parallelization}},
url = {http://www.cs.virginia.edu/~skadron/Papers/yang\_hotpar11.pdf},
year = {2011}
}
@inproceedings{Holk2011,
abstract = {The recent rise in the popularity of Graphics Processing Units (GPUs) has been fueled by software frameworks, such as NVIDIA’s Compute Unified De- vice Architecture (CUDA) and Khronos Group’s OpenCL that make GPUs avail- able for general purpose computing. However, CUDA and OpenCL are still low- level approaches that require users to handle details about data layout and move- ment across levels of memory hierarchy. We propose a declarative approach to coordinating computation and data movement between CPU and GPU, through a domain-specific language that we called Harlan. Not only does a declarative lan- guage obviate the need for the programmer to write low-level error-prone boiler- plate code, by raising the abstraction of specifying GPU computation it also allows the compiler to optimize data movement and overlap between CPU and GPU com- putation. By focusing on the “what”, and not the “how”, of data layout, data move- ment, and computation scheduling, the language eliminates the sources of many programming errors related to correctness and performance.},
address = {Bloomington},
author = {Holk, E and Byrd, WE and Mahajan, Nilesh and Willcock, Jeremiah},
booktitle = {Parallel Computing},
editor = {Boss, Deschere and D'hollander, Erik H. and Joubert, Gerhard R. and Padua, David and Peters, Frans and Sawyer, Mark},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Holk et al. - 2011 - Declarative Parallel Programming for GPUs.pdf:pdf},
keywords = {compilers,declarative parallel programming,gpgpus},
pages = {297--304},
publisher = {IOS Press},
title = {{Declarative Parallel Programming for GPUs.}},
url = {http://osl.iu.edu/publications/prints/2011/2011-parco-holk-harlan.pdf},
volume = {22},
year = {2011}
}
@article{Patt1997,
abstract = {Billion-transistor processors will be much as they are today, just bigger, faster and wider (issuing more instructions at once). The authors describe the key problems (instruction supply, data memory supply and an implementable execution core) that prevent current superscalar computers from scaling up to 16- or 32-instructions per issue. They propose using out-of-order fetching, multi-hybrid branch predictors and trace caches to improve the instruction supply. They predict that replicated first-level caches, huge on-chip caches and data value speculation will enhance the data supply. To provide a high-speed, implementable execution core that is capable of sustaining the necessary instruction throughput, they advocate a large, out-of-order-issue instruction window (2,000 instructions), clustered (separated) banks of functional units and hierarchical scheduling of ready instructions. They contend that the current uniprocessor model can provide sufficient performance and use a billion transistors effectively without changing the programming model or discarding software compatibility},
author = {Patt, Y.N. and Patel, S.J. and Evers, M. and Friendly, D.H. and Stark, J.},
doi = {10.1109/2.612249},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Patt et al. - 1997 - One billion transistors, one uniprocessor, one chip.html:html},
issn = {00189162},
journal = {Computer},
number = {9},
pages = {51--57},
title = {{One billion transistors, one uniprocessor, one chip}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=612249},
volume = {30},
year = {1997}
}
@article{Kiczales2001,
abstract = {AspectJ is a simple and practical aspect-oriented extension to Java.. With just a few new constructs, AspectJ provides support for modular implementation of a range of crosscutting concerns. In AspectJs dynamic join point model, join points are well-defined points in the execution of the program; pointcuts are collections of join points; advice are special method-like constructs that can be attached to pointcuts; and aspects are modular units of crosscutting implementation, comprising pointcuts, advice, and ordinary Java member declarations. AspectJ code is compiled into standard Java bytecode. Simple extensions to existing Java development environments make it possible to browse the crosscutting structure of aspects in the same kind of way as one browses the inheritance structure of classes. Several examples show that AspectJ is powerful, and that programs written using it are easy to understand.},
author = {Kiczales, Gregor and Hilsdale, Erik and Hugunin, Jim and Kersten, Mik and Palm, Jeffrey and Griswold, William G},
doi = {10.1007/3-540-45337-7\_18},
isbn = {3540422064},
issn = {03029743},
journal = {Main},
pages = {327--353},
title = {{An Overview of AspectJ}},
url = {http://www.cs.ubc.ca/~kdvolder/binaries/aspectj-overview.pdf},
volume = {2072},
year = {2001}
}
@article{Wang2009,
author = {Wang, Cheng and Wu, Youfeng and Borin, Edson and Hu, Shiliang and Liu, Wei and Sager, Dave and Ngai, Tin-fook and Fang, Jesse},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2009 - Dynamic Parallelization of Single-Threaded Binary Programs using Speculative Slicing.pdf:pdf},
isbn = {9781605584980},
title = {{Dynamic Parallelization of Single-Threaded Binary Programs using Speculative Slicing}},
year = {2009}
}
@misc{Clarke2011,
author = {Clarke, Trent},
booktitle = {for fun() and profit},
month = oct,
title = {{Why Functional?}},
url = {http://forfunand.wordpress.com/2011/10/07/why-functional},
year = {2011}
}
@inproceedings{Allan2005,
abstract = {Abc is an extensible, optimising compiler for AspectJ. It has been designed as a workbench for experimental research in aspect-oriented programming languages and compilers. We outline a programme of research in these areas, and we review how abc can help in achieving those research goals},
author = {Allan, Chris and Avgustinov, Pavel and Christensen, Aske Simon and Hendren, Laurie J and Kuzins, Sascha and Lhot\'{a}k, Jennifer and Lhot\'{a}k, Ondrej and {De Moor}, Oege and Sereni, Damien and Sittampalam, Ganesh and Tibble, Julian},
booktitle = {Generative Programming and Component Engineering 4th International Conference GPCE},
doi = {10.1007/11561347\_2},
isbn = {3540291385},
pages = {10--16},
title = {{abc: The AspectBench Compiler for AspectJ}},
volume = {3676},
year = {2005}
}
@misc{GroovyDocs,
author = {Strachan, James and {The Groovy Project}},
booktitle = {Groovy API documentation},
title = {org.codehaus.groovy.classgen.asm},
url = {http://groovy.codehaus.org/gapi/org/codehaus/groovy/classgen/asm/package-summary.html},
year = {2013}
}
@techreport{Mueller2009,
address = {New York, New York, USA},
author = {Mueller, Klaus},
file = {:afs/inf.ed.ac.uk/user/s12/s1255753/Documents/\_mendeley/Mueller/Unknown/Mueller - 2009 - GPU Programming CUDA Threads - Unknown.pdf:pdf},
institution = {Stony Brook University},
pages = {2007--2009},
title = {{GPU Programming: CUDA Threads}},
url = {http://www.cs.sunysb.edu/~mueller/teaching/cse591\_GPU/threads.pdf},
year = {2009}
}
@book{TAOCPvol2,
author = {Knuth, Donald E.},
edition = {Third},
isbn = {0-201-89683-2},
publisher = {Addison Wesley},
title = {{The Art of Computer Programming Volume 2: Seminumerical Algorithms}},
year = {1997}
}
@misc{Yang1997,
abstract = {Equijoin between two relations is one of the basic operations in relational database and a large volume of research have been devoted to it. However, in recent years, there hasn't been a survey which objectively compares a wide spectrum of various join techniques in their relative performances. This survey compares performance and practicality between various join techniques. Main criteria for performance comparisons are disk I/Os. For comparing of practicality, criteria used are easiness and flexibility of implementation. When comparing join techniques, each join technique is evaluated in its full potential, which means that if other techniques are available to enhance this join technique while retaining the main philosophy of it, the later techniques are applied. The main contribution of the paper is that it confirms the believe that no dramatical performance improvement of three major join techniques (nested loops, sort-based, and hash based) of relational database can},
author = {Yang, Yuping and Singhal, Mukesh},
booktitle = {Main},
keywords = {band join,centralized,equijoin,join,join index,join methods,monoobjective,one-shot,optimization,query execution,relational,relational database,survey},
mendeley-tags = {centralized,join methods,monoobjective,one-shot,optimization,relational,survey},
title = {{A comprehensive survey of join techniques in relational databases}},
year = {1997}
}
@book{DatabasesBook,
author = {Garcia-Molina, Hector and Ullman, Jeffrey D. and Widom, Jennifer},
edition = {First},
isbn = {0-13-031995-3},
publisher = {Prentice Hall},
title = {{Database Systems: The Complete Book}},
year = {2002}
}
@misc{JAMM,
author = {Ellis, Jonathan},
booktitle = {GitHub},
title = {{Java Agent for Memory Measurements}},
url = {https://github.com/jbellis/jamm/},
urldate = {28/07/2013},
year = {2011}
}
@book{ArchitectureBook,
address = {Harlow},
author = {Stallings, William},
edition = {Nineth},
editor = {Horton, Marcia and Dunkelberger, Tracy and Snyder, Carole and Holcomb, Jeff and Arunachalam, Jayashree},
isbn = {0-273-76919-7},
publisher = {Pearson},
title = {{Computer Organisation and Architecture: Design for Performance}},
year = {2013}
}
@article{Bruneton2002,
abstract = {ASM is a Java class manipulation tool designed to dynamically generate and manipulate Java classes, which are useful techniques to implement adaptable systems. ASM is based on a new approach, compared to equivalent existing tools, which consists in using the \&quot;visitor\&quot; design pattern without explicitly representing the visited tree with objects. This new approach gives much better performances than those of existing tools, for most of practical needs.},
author = {Bruneton, Eric and Lenglet, Romain and Coupaye, Thierry},
doi = {10.1.1.117.5769},
journal = {Adaptable and extensible component systems},
keywords = {adaptability,adaptabilit\'{e},class,classe,code,dynamic,dynamique,g\'{e}n\'{e}ration,java,transformation,visiteur,visitor},
title = {{ASM: a code manipulation tool to implement adaptable systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.5769},
volume = {30},
year = {2002}
}
@article{JSR133,
author = {{Oracle Corporation}},
file = {:afs/inf.ed.ac.uk/user/s12/s1255753/Documents/\_mendeley/Oracle Corporation/Unknown/Oracle Corporation - 2004 - JSR-133 Java Memory Model and Thread Specification - Unknown.pdf:pdf},
title = {{JSR-133: Java Memory Model and Thread Specification}},
url = {http://www.cs.umd.edu/~pugh/java/memoryModel/jsr133.pdf},
year = {2004}
}
@misc{ApacheBECL,
author = {{Apache Foundation}},
title = {{Apache Bytecode Engineering Library}},
url = {http://commons.apache.org/proper/commons-bcel/},
urldate = {30/07/2013},
year = {2013}
}
@techreport{Marshall2005,
address = {Santa Cruz},
author = {Marshall, Casey},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Marshall - 2005 - Software Transactional Memory.pdf:pdf},
institution = {University of California, Santa Cruz},
title = {{Software Transactional Memory}},
url = {http://www.cs.uic.edu/~ajayk/STM.pdf},
year = {2005}
}
@article{Backus1979,
abstract = {This article discusses attitudes about "automatic programming," the economics of programming, and existing programming systems, all in the early 1950s. It describes the formation of the FORTRAN group, its knowledge of existing systems, its plans for FORTRAN, and the development of the language in 1954. It describes the development of the optimizing compiler for FORTRAN I, of various language manuals, and of FORTRAN II and III. It concludes with remarks about later developments and the impact of FORTRAN and its successors on programming today.},
author = {Backus, John},
doi = {10.1109/MAHC.1979.10013},
issn = {1058-6180},
journal = {IEEE Annals of the History of Computing},
month = jan,
number = {1},
pages = {21--37},
title = {{The History of FORTRAN I, II and III}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4392880},
volume = {1},
year = {1979}
}
@article{Tournavitis2009,
author = {Tournavitis, Georgios and Wang, Zheng and Franke, Bj\"{o}rn and O'Boyle, Michael F.P.},
doi = {10.1145/1543135.1542496},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Tournavitis et al. - 2009 - Towards a holistic approach to auto-parallelization.pdf:pdf},
isbn = {978-1-60558-392-1},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {auto-parallelization,machine-learning based parallelism mapping,openmp,profile-driven parallelism detection},
month = may,
number = {6},
pages = {177},
title = {{Towards a holistic approach to auto-parallelization}},
url = {http://dl.acm.org/citation.cfm?id=1543135.1542496},
volume = {44},
year = {2009}
}
@inproceedings{Rahnavard2013,
author = {Rahnavard, Gholamali and Cook, Jonathan},
booktitle = {11th International Workshop on Dynamic Analysis},
file = {:Volumes/Acme/Downloads/Rahnavard.pdf:pdf},
keywords = {aspect oriented programming,runtime monitoring},
title = {{An Extensible AOP Framework for Runtime Monitoring}},
url = {http://web.eecs.umich.edu/~nsatish/woda-2013/Rahnavard.pdf},
year = {2013}
}
@techreport{Atkin2012,
address = {Keele},
author = {Atkin-Granville, Christopher Edward},
institution = {University of Keele},
title = {{Visual Cues for CT Image Segmentation}},
year = {2012}
}
@article{Franz2008,
author = {Franz, Michael},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Franz - 2008 - Dynamic Parallelization and Vectorization of Binary Executables on Hierarchical Platforms.pdf:pdf},
pages = {1--24},
title = {{Dynamic Parallelization and Vectorization of Binary Executables on Hierarchical Platforms}},
volume = {10},
year = {2008}
}
@techreport{Constantinides2004,
abstract = {In his famous letter Go To statement considered harmful Dijkstra started a discussion finally resulting in banning of most unstructured control flow statements from modern high level programming languages. To overcome limitations of todays programming languages, aspect oriented programming has been proposed. Unfortunately language elements used by many AO languages are in a way similar to the Go To statment, so we ask the provocative: AOP considered harmful?},
author = {Constantinides, C and Skotiniotis, T and Stoerzer, M},
booktitle = {1 st European Interactive Workshop on Aspect Systems EIWAS},
institution = {Concordia University},
title = {{AOP considered harmful}},
url = {http://pp.info.uni-karlsruhe.de/uploads/publikationen/constantinides04eiwas.pdf},
year = {2004}
}
@misc{CTcoursework,
address = {Edinburgh},
author = {Franke, Bj\"{o}rn},
publisher = {University of Edinburgh},
title = {{Compiling Techniques coursework}},
url = {http://www.inf.ed.ac.uk/teaching/courses/ct/Coursework/Coursework\_2013.pdf},
year = {2013}
}
@inproceedings{Harbulot2005,
abstract = {The current AspectJ join points represent locations in the code that are the interface of the Java objects. However, not all the "things that happen" happen at the interfaces. In particular, loops are a key place that could be advised for parallelisation. This article presents a loop join point model, which allows AspectJ to intervene directly in loops. More generally, this demonstrates the need for, and provides, a more complex join point in AspectJ.The approach used for recognising loops is based on a control-flow analysis at the bytecode level; this avoids ambiguities due to alternative forms of source-code that would effectively produce identical loops. This model is also enhanced with a mechanism for context exposure, which is pivotal for giving a meaning to the use of this join point, and with additional information through join point reflection. The context exposure is particularly useful for writing pointcuts that select specific loops only, and the problem of loop selection is also presented in the paper.Finally, LoopsAJ, an extension for the abc compiler that provides AspectJ with a loop join point, is presented. It is shown how to use this extension for writing aspects that parallelise loops.},
author = {Harbulot, Bruno and Gurd, John R},
booktitle = {Computer},
doi = {10.1145/1119655.1119666},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Harbulot, Gurd - 2005 - A join point for loops in AspectJ.pdf:pdf},
isbn = {159593300X},
pages = {63--74},
title = {{A join point for loops in AspectJ}},
year = {2005}
}
@book{DragonBook,
author = {Aho, Alfred V. and Lam, Monica S. and Sethi, R and Ullman, Jeffrey D.},
edition = {Second},
isbn = {0-321-49169-6},
publisher = {Pearson},
title = {{Compilers: Principles, Techniques and Tools}},
year = {2007}
}
@inproceedings{Liu2011,
address = {New York, New York, USA},
author = {Liu, Tongping and Curtsinger, Charlie and Berger, Emery D.},
booktitle = {Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles - SOSP '11},
doi = {10.1145/2043556.2043587},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Curtsinger, Berger - 2011 - Dthreads.pdf:pdf},
isbn = {9781450309776},
month = oct,
pages = {327},
publisher = {ACM Press},
title = {{Dthreads}},
url = {http://dl.acm.org/citation.cfm?id=2043556.2043587},
year = {2011}
}
@misc{RuntimeDocs,
author = {{Oracle Inc}},
booktitle = {Java 7 documentation},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Oracle Inc - 2013 - Runtime (Java Platform SE 7).html:html},
title = {{Runtime (Java Platform SE 7)}},
url = {http://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.html},
urldate = {25/07/13},
year = {2013}
}
@article{Gregg2003,
abstract = {In this paper we present a platform independent analysis of the dynamic profiles of Java programs when executing on the Java Virtual Machine. The Java programs selected are taken from the Java Grande Forum benchmark suite, and five different Java-to-bytecode compilers are analysed. The results presented describe the dynamic instruction usage fre- quencies, as well as the sizes of the local variable, parameter and operand stacks during execution on the JVM. These results, presenting a picture of the actual (rather than presumed) behaviour of the JVM, have implications both for the coverage aspects of the Java Grande benchmark suites, for the performance of the Java-to-bytecode compilers, and for the design of the JVM.},
author = {Gregg, David and Power, James and Waldron, John},
doi = {10.1002/cpe.666},
issn = {15320626},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {java grande,java virtual machine},
month = mar,
number = {35},
pages = {459--484},
title = {{Platform independent dynamic Java virtual machine analysis: the Java Grande Forum benchmark suite}},
url = {http://doi.wiley.com/10.1002/cpe.666},
volume = {15},
year = {2003}
}
@phdthesis{Schwaighofer2009,
author = {Schwaighofer, Arnold},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Schwaighofer - 2009 - Tail Call Optimization in the Java HotSpot ™ VM Diplom-Ingenieur.pdf:pdf},
school = {Johannes Kepler University Linz},
title = {{Tail Call Optimization in the Java HotSpot ™ VM Diplom-Ingenieur}},
type = {Masterstudium Informatik thesis},
url = {http://www.ssw.uni-linz.ac.at/Research/Papers/Schwaighofer09Master/schwaighofer09master.pdf},
year = {2009}
}
@article{Raman2012,
author = {Raman, Raghavan and Zhao, Jisheng and Sarkar, Vivek and Vechev, Martin and Yahav, Eran},
doi = {10.1145/2345156.2254127},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Raman et al. - 2012 - Scalable and precise dynamic datarace detection for structured parallelism.pdf:pdf},
isbn = {978-1-4503-1205-9},
issn = {0362-1340},
journal = {ACM SIGPLAN Notices},
keywords = {data races,parallelism,program analysis},
month = aug,
number = {6},
pages = {531--531--542--542},
publisher = {ACM},
title = {{Scalable and precise dynamic datarace detection for structured parallelism}},
url = {http://dl.acm.org/citation.cfm?id=2345156.2254127},
volume = {47},
year = {2012}
}
@misc{Ibbett2009,
author = {Ibbett, Ronald},
title = {{High Performance Computer Architectures}},
url = {http://homepages.inf.ed.ac.uk/cgi/rni/comp-arch.pl?Paru/depend.html,Paru/depend-f.html,Paru/menu.html},
urldate = {30/07/2013},
year = {2009}
}
@unpublished{Anderson2013,
abstract = {Papers on functional language implementations frequently set the goal of achieving performance “comparable to C”, and sometimes report results comparing benchmark results to concrete C imple- mentations of the same problem. A key pair of questions for such comparisons is: what C program to compare to, and what C com- piler to compare with? In a 2012 paper, Satish et al [9] compare naive serial C implementations of a range of throughput-oriented benchmarks to best-optimized implementations parallelized on a six-core machine and demonstrate an average 23× (up to 53×) speedup. Further, they demonstrate that most of this so-called “Ninja-gap” between naive C and best-optimized code can be elim- inated using relatively straightforward C programming techniques. Even accounting for thread parallel speedup, these results demon- strate a substantial performance gap between naive and tuned C code. In this paper we choose a subset of the benchmarks studied by Satish et al to port to Haskell. We measure performance of these Haskell benchmarks compiled with the standard Glasgow Haskell Compiler and with our experimental Intel Labs Haskell Research Compiler and report results as compared to our best reconstruc- tions of the algorithms used by Satish et al. Results are reported as measured both on an Intel Xeon E5-4650 32-core machine, and on an Intel Xeon Phi co-processor. We hope that this study provides valuable data on the concrete performance of Haskell relative to C.},
author = {Anderson, Todd A and Petersen, Leaf and Lui, Hai and Glew, Neal},
booktitle = {Haskell Symposium 2013},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Anderson et al. - 2013 - Measuring the Haskell Gap.pdf:pdf},
pages = {1--12},
title = {{Measuring the Haskell Gap}},
url = {http://www.leafpetersen.com/leaf/publications/hs2013/haskell-gap.pdf},
year = {2013}
}
@incollection{Latorre2011,
author = {Latorre, F and Magklis, G and Gonz\'{a}lez, J},
booktitle = {Transactions on High-Performance Embedded Architectures and Compilers III},
doi = {10.1007/978-3-642-19448-1\_7},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Latorre, Magklis, Gonz\'{a}lez - 2011 - CROB implementing a large instruction window through compression.bin:bin},
isbn = {978-3-642-19447-4},
pages = {115--134},
publisher = {Springer Berlin Heidelberg},
title = {{CROB: implementing a large instruction window through compression}},
url = {http://www.springerlink.com/index/9371360L17X6240W.pdf},
year = {2011}
}
@inproceedings{Marek2012,
address = {New York, New York, USA},
author = {Marek, Luk\'{a}\v{s} and Villaz\'{o}n, Alex and Zheng, Yudi and Ansaloni, Danilo and Binder, Walter and Qi, Zhengwei},
booktitle = {Proceedings of the 11th annual international conference on Aspect-oriented Software Development - AOSD '12},
doi = {10.1145/2162049.2162077},
file = {:Users/chris/Library/Application Support/Mendeley Desktop/Downloaded/Marek et al. - 2012 - DiSL.pdf:pdf},
isbn = {9781450310925},
keywords = {JVM,aspect-oriented programming,bytecode instrumentation,dynamic program analysis},
month = mar,
pages = {239},
publisher = {ACM Press},
title = {{DiSL}},
url = {http://dl.acm.org/citation.cfm?id=2162049.2162077},
year = {2012}
}
