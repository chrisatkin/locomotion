\chapter{Methodology} \label{chp:methodology}
\section{Introduction} \label{sec:methodology/introduction}
In this chapter, the experimental set up and technique is outlined, as well as the specific benchmarks that were used. Rationales for each benchmark are also presented.

\section{Experimental Setup} \label{sec:methodology/setup}
The hardware used for the experiments is a mid-2009 MacBook Pro with the following specifications:
	
\begin{itemize}
	\item Intel Core 2 Duo P8800 @ 2.66GHz
	\item 8GB 1067MHz DDR3
	\item Nvidia GeForce 9600M GT 256MB
	\item Disks:
	\begin{itemize}
		\item Samsung SSD 830 Series 128GB via SATA-2
		\item Western Digital Scorpio Blue 1TB via SATA-2
	\end{itemize}
\end{itemize}

The software configuration is as follows:
\begin{itemize}
	\item OS X 10.8.4 (build 12E55)
	\item Java 7 update 25
\end{itemize}

All software used was the latest version available at the time of writing.

\textit{In silico} experimental design is notoriously difficult within the context of standard scientific procedure. Reproducibility of experiments is not always guaranteed, as slight and sometimes un-noticed machine intricacies and flaws can affect experimental results. Because of this, all benchmarks were run on a separate user account (in order to reduce the number of running processes which may affect results, perhaps by consuming additional resources).

\section{Repeats} \label{sec:methodology/repeats}
In order to improve the results, each experiment was repeat three times. The average of the three repeats was used for the rest of the analyses. In doing so, the number of dependencies between repeats was checked.

\section{Benchmarks} \label{sec:methodology/benchmarks}
	\subsection{Validation, Basic Testing and Parametric Testing} \label{sec:methodology/benchmarks/simple}
	These benchmarks fall under the same category, and are covered in chapter \ref{chp:parametric}.
	
	The first `item of business' is to prove that the implementation of the instrumentation and runtime library is correct. We ensure this by comparing the results of algorithms with results known ahead-of-schedule with the results of experiments.
	
	Since there are several kinds of dependency (see figure \ref{sec:runtime/analysis}), these benchmarks need to take this into account.
	
	The way these experiments work is by generating three arrays:
	
	\begin{itemize}
		\item \textbf{Value arrays}: since the basic memory operation is that of \texttt{a[b[i]]} (where \texttt{a} and \texttt{b} are both arrays), these must both be generated. These arrays are randomly generated, but the parameters
		\item \textbf{Operation array} 
	\end{itemize}
	
	\subsection{Parametric Benchmarks} \label{sec:methodology/benchmarks/parametric}
	In addition to basic verification, a parametric benchmark has been created. This benchmark -- called \textsc{HazardMark-II} -- allows a user to specify the number of each kind of dependency. This was used to determine the relative overheads of the analysis.
	
	\subsection{Graph Processing Algorithms} \label{sec:methodology/benchmarks/graphs}
	Many graph processing algorithms are known to be parallelisable in the sense that each node can, in some cases, be operated upon in parallel. An example of such an algorithm would be a in a ray-tracer. Scene data is stored as a graph (\eg, the top level node is the scene, with sub-scenes occuring). Ray-tracing is an embarrassingly parallel problem (in that each ray can be traced without being dependent on other rays). Hence, the scene graph must be at-least traversable in parallel.
	
	Another example is that of divide-and-conquer algorithms. This class of algorithm uses recursive decomposition in order to `break-down' tasks into smaller sub-tasks which can be computed in parallel. These tasks are represented as a tree, which are undirected graphs. At a given recursion depth, each task is computed in parallel and then merged (also in parallel). This cycle continues until all sub-tasks have been merged.
	
	However, not all graph algorithms are parallelisable.
	
	In this benchmark, we consider several graph algorithms from the JGraphT project. The algorithms which we consider are:
	
	\begin{itemize}
		\item Blah.
	\end{itemize}
	
	\subsection{Java Grande} \label{sec:methodology/benchmarks/grande}
	Java Grande \citep{Smith2001,Bull2001} is a platform-independent benchmark for Java Virtual Machines and their associated compilers. Indeed, it is aimed at measuring the performance of the \emph{virtual machine}, rather than the Java language.
	
	The authors cite a \textit{grande application} as one that ``uses large amounts of processing, I/O, network bandwidth or memory''.
	Each test runs in three sizes: A, B and C. The tests which are are including are:
	
	\begin{itemize}
		\item \textbf{Fast Fourier Transform}: one-dimensional transform of $n$ complex numbers. FFT uses complex arithmetic, 
	\end{itemize}
	
	\subsection{N-Body Simulation} \label{sec:methodology/benchmarks/nbody}
	N-Body simulations \citep{Trenti2008} are computational simulations of real-world physical systems. They simulate a number ($N$) of particles (although in this context a particle does not need to be very small as in particle physics), acting under some forces (usually gravity).
	
	The N-Body problem was chosen because it is known to be computable in parallel \citep{Warren1993,Nyland2007}. The program used for these benchmarks is available from Princeton University\footnote{\url{http://introcs.cs.princeton.edu/java/34nbody/Universe.java.html}}, although it has been slightly modified by fellow Master of Science student Ranjeet Singh. In the modified version, computation of forces has been vectorised, rather than by calling a method on the \texttt{Vector} class. The benchmark will focus on a this vectorisation.
	
\section{Measurement Methodology} \label{sec:methodology/measurements}
	\subsection{Execution Time} \label{sec:methodology/measurements/time}
	Execution time was measured by taking the difference between \texttt{System.nanoTime()} before and after the experiment was run. This is superior to using other methods, such as the Unix \texttt{time} program because it computes an accurate value, instead of elapsed user-space CPU time.
	
	\subsection{Memory Usage} \label{sec:methodology/measurements/memory}
	The difficulties of measuring memory usage in Java programs due to the non-deterministic nature of the garbage collector are well documented in the literature \citep{Kim2000,Ogata2010}. Despite this, the Java 7 API presents several techniques \citep{RuntimeDocs} of measuring memory within the JVM:
	
	\begin{itemize}
		\item \texttt{freeMemory()}: the amount of free memory in the virtual machine
		\item \texttt{maxMemory()}: the maximum amount of memory that the virtual machine will attempt to use
		\item \texttt{totalMemory()}: the amount of memory currently in use by the virtual machine
	\end{itemize}
	
	In addition, there is a Java Agent for measuring memory usage of an object - the Java Agent for Memory Measurements \citep{JAMM} (JAMM). JAMM is essentially a wrapper for the \texttt{java.lang.instrument.Instrumentation.getObjectSize()} method. There are several methods available, and the framework uses \texttt{measureDeep()} for the greatest accuracy.
	
	\texttt{measureDeep()} crawls the object graph, calling \texttt{getObjectSize()} on each object it encounters. An \texttt{IdentityHashMap} is used to detect loops in the object graph. Unfortunately, this does affect execution time - but memory usage is recorded after execution time has been recorded. \citeauthor{JAMM} does suggest investigating the possible use of bloom filters to overcome this memory usage, but this it outside the scope of this project.
	
\section{Test Harness Design and Implementation} \label{sec:methodology/implementation}
	
\section{Summary} \label{sec:methodology/summary}