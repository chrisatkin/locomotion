\chapter{Methodology} \label{chp:methodology}
\section{Introduction} \label{sec:methodology/introduction}
In this chapter, the experimental set up and technique is outlined, as well as the specific benchmarks that were used. Rationales for each benchmark are also presented.

\section{Experimental Setup} \label{sec:methodology/setup}
The hardware used for the experiments is a mid-2009 MacBook Pro with the following specifications:
	
\begin{itemize}
	\item Intel Core 2 Duo P8800 @ 2.66GHz
	\item 8GB 1067MHz DDR3
	\item Nvidia GeForce 9600M GT 256MB
	\item Disks:
	\begin{itemize}
		\item Samsung SSD 830 Series 128GB via SATA-2
		\item Western Digital Scorpio Blue 1TB via SATA-2
	\end{itemize}
\end{itemize}

The software configuration is as follows:
\begin{itemize}
	\item OS X 10.8.4 (build 12E55)
	\item Java 7 update 25
\end{itemize}

All software used was the latest version available at the time of writing.

\textit{In silico} experimental design is notoriously difficult within the context of standard scientific procedure. Reproducibility of experiments is not always guaranteed, as slight and sometimes un-noticed machine intricacies and flaws can affect experimental results. Because of this, all benchmarks were run on a separate user account (in order to reduce the number of running processes which may affect results, perhaps by consuming additional resources).

\section{Repeats} \label{sec:methodology/repeats}
In order to improve the results, each experiment was repeat three times. The average of the three repeats was used for the rest of the analyses. In doing so, the number of dependencies between repeats was checked.

\section{Benchmarks} \label{sec:methodology/benchmarks}
	\subsection{Validation and Basic Testing} \label{sec:methodology/benchmarks/simple}
	The first `item of business' is to prove that the implementation of the instrumentation and runtime library is correct. We ensure this by comparing the results of algorithms with results known ahead-of-schedule with the results of experiments.
	
	Since there are several kinds of dependency (see figure \ref{sec:runtime/analysis}), these benchmarks need to take this into account.
	
	\subsection{Parametric Benchmarks} \label{sec:methodology/benchmarks/parametric}
	In addition to basic verification, a parametric benchmark has been created. This benchmark -- called \textsc{HazardMark-II} -- allows a user to specify the number of each kind of dependency. This was used to determine the relative overheads of the analysis.
	
	\subsection{Graph Processing Algorithms} \label{sec:methodology/benchmarks/graphs}
	
	\subsection{Java Grande} \label{sec:methodology/benchmarks/grande}
	Java Grande \citep{Smith2001,Bull2001} is a platform-independent benchmark for Java Virtual Machines and their associated compilers. Indeed, it is aimed at measuring the performance of the \emph{virtual machine}, rather than the Java language.
	
	The authors cite a \textit{grande application} as one that ``uses large amounts of processing, I/O, network bandwidth or memory''. The benchmarks that are included in the Grande suite are:
	
	\begin{itemize}
		\item \textbf{euler} solves a set of equations using a fourth-order Runge-Kutta method
		\item \textbf{moldyn} compute molecular; it is a Java port of a program originally written in Fortran, for this reason it does not use object-oriented programming techniques.
		\item \textbf{montecarlo} is a financial simulation based on Monte-Carlo methods
		\item \textbf{raytracer} computes a scene containing 64 spheres
		\item \textbf{search} solves a game of Connect4
	\end{itemize}
	
	\subsection{N-Body Simulation} \label{sec:methodology/benchmarks/nbody}
	N-Body simulations \citep{Trenti2008} are computational simulations of real-world physical systems. They simulate a number ($N$) of particles (although in this context a particle does not need to be very small as in particle physics), acting under some forces (usually gravity).
	
	The N-Body problem was chosen because it is known to be computable in parallel \citep{Warren1993,Nyland2007}. The program used for these benchmarks is available from Princeton University\footnote{\url{http://introcs.cs.princeton.edu/java/34nbody/Universe.java.html}}, although it has been slightly modified by fellow Master of Science student Ranjeet Singh. In the modified version, computation of forces has been vectorised, rather than by calling a method on the \texttt{Vector} class. The benchmark will focus on a this vectorisation.
	
\section{Measurement Methodology} \label{sec:methodology/measurements}
	\subsection{Execution Time} \label{sec:methodology/measurements/time}
	Execution time was measured by taking the difference between \texttt{System.nanoTime()} before and after the experiment was run. This is superior to using other methods, such as the Unix \texttt{time} program because it computes an accurate value, instead of elapsed user-space CPU time.
	
	\subsection{Memory Usage} \label{sec:methodology/measurements/memory}
	The difficulties of measuring memory usage in Java programs due to the non-deterministic nature of the garbage collector are well documented in the literature \citep{Kim2000,Ogata2010}. Despite this, the Java 7 API presents several techniques \citep{RuntimeDocs} of measuring memory within the JVM:
	
	\begin{itemize}
		\item \texttt{freeMemory()}: the amount of free memory in the virtual machine
		\item \texttt{maxMemory()}: the maximum amount of memory that the virtual machine will attempt to use
		\item \texttt{totalMemory()}: the amount of memory currently in use by the virtual machine
	\end{itemize}
	
	In addition, there is a Java Agent for measuring memory usage of an object - the Java Agent for Memory Measurements \citep{JAMM} (JAMM). JAMM is essentially a wrapper for the \texttt{java.lang.instrument.Instrumentation.getObjectSize()} method. There are several methods available, and the framework uses \texttt{measureDeep()} for the greatest accuracy.
	
	\texttt{measureDeep()} crawls the object graph, calling \texttt{getObjectSize()} on each object it encounters. An \texttt{IdentityHashMap} is used to detect loops in the object graph. Unfortunately, this does affect execution time - but memory usage is recorded after execution time has been recorded. \citeauthor{JAMM} does suggest investigating the possible use of bloom filters to overcome this memory usage, but this it outside the scope of this project.
	
\section{Test Harness Design and Implementation} \label{sec:methodology/implementation}
	
\section{Summary} \label{sec:methodology/summary}