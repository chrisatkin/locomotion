Conclusion chp:conclusion
Concluding Remarks sec:conclusion/remarks

Unsolved Problems sec:conclusion/unsolved

Future Work sec:conclusion/future-workThe Graal Compiler Infrastructure chp:graal
Background sec:graal/background
The basis of the project is the Graal compiler infrastructure Graal. Graal is an experimental project developed mainly at Oracle Labs (although there are some additional collaborators at AMD) under the OpenJDK programme.
	
The aim of the Graal project is to develop a Java compiler written in Java itself - `a quest for the JVM to leverage its own J'. Graal is, in essence, a Java compiler written in Java. However, this doesn't fully explain the Graal project.
	
Virtually all languages used commonly in industry have had their compilers go through a so-called `bootstrap' process. This bootstrapping process involves writing the compiler for a language in the language it is intended to compile. In many cases the first version of a compiler is written in a different language - commonly used languages include the standard C and C++ due to their performance. There are many examples of this in the real-world - GCC is written in a combination of C and C++ Although the project is currently converting all C code to C++, LLVM/Clang is written in C++ etc. There are several advantages to this approach - in essence, this process is a kind of informal proof that the language has matured to a level that is capable of supporting a program as complicated as a compiler. In effect, bootstrapping a compiler displays shows that a language (and associated platform) has a certain level of `maturity', that it is now ready for large software projects (or at least not totally unprepared for one).

Graal is an attempt to bring this approach to the Java language. Note that there are other projects with attempts to bootstrap parts of the Java platform - for example, the Maxine VM is a Java virtual machine written in Java. However, because Java is unlike most other platforms (in that it not only requires a compiler, but also a virtual machine, class library and such), the compiler has, until the advent of the Graal project, remained written in C++.

Another feature of Graal is that it allows users (of the compiler) to interface directly with the compiler. Common compilers (GCC, ICC etc) are seen as `black-boxes', where a user invokes the compiler, waits for a while, and then a resulting object file or binary is produced. Graal is a part of a new generation of compilers that expose APIs to users, which means users can change parts of the compilation process to suit their needs, ease debugging and other such advantages. With modern languages and platforms being required to target multiple different machine classes (module, desktop, laptop and server/cloud), this is a crucial advantage over more conventional languages and platforms. There are only a few examples of this new generation of compiler, but another - somewhat more mainstream example - is Microsoft's Rosyln project for their .NET platform. The new Windows Runtime include deep metadata integration into the platform (which is the basis for, amongst other things, the Common Object System in .NET languages Which allows inter-language types to be considered equivalent - a C int is semantically equivalent to a C++ int, a C int, a JavaScript int and so on).

Introduction sec:graal/introduction
Graal is somewhat different than other compilers. As opposed to other compilers, which use a combination of parsers and lexers to produce their IRs from source code, Graal builds the IR from Java bytecode instead. This approach has several advantages for this project, the main being that we cannot assume that the source code is available to many legacy programs. Another advantage to this approach is that it would allow the detection mechanism to not only be performed upon user-provided programs, but also system-level libraries as well (for example, the Java Collections Framework). 

Intermediate Representations sec:graal/ir
Like many compilers, Graal uses several different internal representations at different stages of compilation. Each of the representations has a distinct, non-overlapping use case; despite this the graphs are somewhat similar in structure.

As is common in many compilers, Graal uses graphs for intermediate representations. These graphs combine several different kinds of graph together into a single form, such as control flow and memory dependency (data flow).

To illustrate this, consider the figure fig:vector-inline, created using the Ideal Graph Visualiser (IGV).

figure
	
	graphics/loop-inline.png
	Graal HIR created for a vector addition using two array literals
	fig:vector-inline
figure

The format for graph visualisations is the following:
itemize
	Red edges represent control flow
	Blue edges represent memory dependence
	Black edges are defined as edges which are not control flow or memory dependence. In reality, they are mainly used for associating FrameState nodes where appropriate
itemize

The text inside nodes uses the following format:

<node-id> NodeName <additional>

In some cases, <additional> contains the value associated with the code (for subtypes of ValueNode). Node IDs are unrelated to the ordering within the graph. Time is represented on the y-axis of the graph, flowing down the page.

fig:vector-inline is a representation of a vector addition using two int array literals as arguments. The original source code contains a loop, which itself contains two array load operations, an addition and an array store operation. From it, we can see clearly the different kinds of node relationships in Graal's IR. The StartNode is the start of the method, which is followed by an allocation (notice that the loop was been optimized away by Graal), the result of which is then returned from the method.

The allocate has two actual memory dependencies:

itemize
	A VirtualArray node is used to create an array. We can see that the type of this array is int[] with length five.
	
	A ConstantNode is referenced five times, one for each position of the array.
itemize

Without both of these dependencies, the compiler cannot create the array, and assign the correct values. Note that, because the JVM allocates default values of zero to declared-yet-unassigned variables, if the ConstantNode was not used, the array would consist of zeros.

Graph Transformations sec:graal/transformations
One of the ways the abilities of Graal are manifested is through it's capacity to apply transformations to the various intermediate representations. The main use for this is to allow users of the compiler to add custom behaviour at the various stages of the compilation process, but the mechanism extends to additional uses. For example, custom behaviour can be inserted into programs, as well as the graphs dumped for inspection in external tools One such tool is the `Ideal Graph Visualiser', which is included in the Graal repository..

As of the time of writing, Graal uses a hybrid approach to transforming graphs between the IRs - suites and phases. They are, in effect, essentially the same thing - they both consist of a sequence of transformations (in Graal terminology, a phase) which are applied to a graph in a well-defined order (although the user can change the ordering if required, as well as disable certain phases via com.oracle.graal.phases.PhasePlan.disablePhase(). The result of a sequence of phases is the representation used at the next lower-level of abstraction. The three phases in Graal are high-level, mid-level, and low-level.

Todo: clear up the difference between runtime-specific lowering and target-specific lowering.

The high-level phase is used mainly for the graph-building phase. Graal uses the concept of ResolvedJavaMethods, which are internal `linkages' to constant pools found within .class files. It is also used for runtime-specific lowering Lowering is a mechanism to convert a complex bytecode into a simpler one, much like the difference between RISC and CISC architectures, for example to handle multiple machine instruction set architectures - although the Java language is unconcerned with JVM implementation details such as the endianness of the machine's CPU, the runtime must, by definition, be aware of this fact. Examples of runtime-specific lowering are found throughout Graal, but examples include multiple implementations of the GraalRuntime interface - one for each ISA that Graal supports. Single Static Assignment (SSA) form is used at the high-level in order to allow for optimisations to be performed.

Mid-level phases remove the SSA form, and includes target-specific lowering. The low-level phases are analogous to the backends of more traditional compilers, and deals with low-level issues such as register allocation and code generation.

	The .class File Format - Constant Pools
	In order to properly understand ResolvedJavaMethods, one needs to understand some parts of the .class file format. The source for this section is the Java Virtual Machine specification [p. 69]JVMSpec.
		
	.class files are structured containers for Java bytecode streams. However, they are not `plain old data structures', as would be indicated by that description. Instead, they are laid out in such a way to increase performance for the JVM.
				
	Unlike some other executable file formats, the JVM does not rely on the (relative) positions of the various kinds of definition permitted. In this context, constants refer to all immutable identifiers - and not simply to the language-level construct of constants (, public static final int THOMASKLAUS = 9;). Each constant has an entry in the constant pool - a table containing cpinfo structures. A ResolvedJavaMethod contains a link to the offset of a cpinfo structure for a method.Instrumentation chp:instrumentation
Graal sec:instrumentation/graal


Bytecode Instrumentation sec:instrumentation/bytecode-instr
	The first approach uses so-called bytecode instrumentation (BCI) - that is, modifying the bytecode directly, either at compile-time or runtime. This approach is advantageous in that arbitrary commands can be inserted, and is only subject to the limitations of the bytecode format. In this sense, arbitrary functionality can be inserted into .class files. However, it is complex (requiring advanced knowledge of the JVM and bytecode formats), as well as being difficult to use. Graal already performs a lot of the work that would be required with this kind of approach, in that it detects control flow and memory dependencies from bytecode. Such systems require a large degree of programmer effort.

	Java Agents sec:instrumentation/bytecode-instr/agents
	At the heart of BCI is the idea of Java Agents javaagents. In order to understand them, however, one must first understand some details of the Java platform.
	
	Unlike some other languages (for example, C and C++ Although note that C and C++ also support dynamic linking), Java is a dynamically linked language. That is to say that the various different libraries (JARs) that Java programs used are linked at run-time, rather than compile-time. The advantage to this approach is that it allows distributables to be smaller in size (recall Java Network Launching Protocol, a method for launching (and therefore, distributing) Java applications over the Internet). The disadvantage to this approach is that it can lead to `dependency hell', although through the combination of versioning metadata in JARs and the extreme backwards compatability mantra in Java, this is not currently a significant issue.
	
	There are three main class loaders in Java:
	
	itemize
		The system class loader loads the classes found in the java.lang package
		Any extensions to Java are loaded via the extension loader
		JARs found within the class path are loaded with the lowest precedence, these include the majority of user-level libraries
	itemize
	
	Java Agents manipulate class files at load-time, through the java.lang.instrument package. The package defines the ClassFileTransformer interface, which provides implementations of class transformers. An advantage of this approach is that since Java Agents are included in the core Java package, developers wishing to use Locomotion would not need to download and install Graal.
	
	There are several different libraries which provide an abstraction layer for such bytecode transformations. The following sections describe the procedure for 

	ASM sec:instrumentation/bytecode-instr/asm
	Despite Java Agents providing the capability to manipulate raw Java bytecode (indeed, the bytecode is made available as a byte array), performing such transformations are difficult and awkward. For this reason, there exists many different libraries for manipulating Java bytecode, ObjectWeb ASM being one of them.
	
	ASM Bruneton2002 is a simple-to-use bytecode manipulation library, itself written in Java. It uses a high-level abstraction for the bytecode, which is advantageous because it allows developers to remain unconcerned with the specifics of control flow analysis, dependency analysis and other such concerns.
	
	Although now common, when ASM was first developed it was considered particularly innovative because it allowed the use of the visitor pattern [p. 331]Gamma1995 for traversing bytecode.

	Javassist sec:instrumentation/alt-instr/bytecode-instr/javassist
	Javassist is similar to ASM but works at a higher level of abstraction.
	
	There are also additional libraries for manipulating Java bytecode, but due to their features (or lack of), they were not considered for this project.

Aspect-Oriented Programming sec:instrumentation/alt-instr/aop
	Aspect-Oriented Programming (AOP) Kiczales1997 is another dialect of object-oriented programming that aims to significantly increase separation of concern within programs, so that programs are more loosely coupled. AOP is a direct descendent from object-oriented programming as well as reflection. Reflection allows programmers to dynamically introspect classes at run-time; changing values and so on. AOP takes this to another level, by allowing so-called advice to be specified (essentially the additional behaviour to be added) and added to join points, which are arbitrary points of control flow within the program.
	
	When combined, aspect-oriented systems add these behaviours to the program in question at compile-time through a process called weaving.
	
	To illustrate this concept, consider the problem of logging method calls. In traditional systems, at each function/method definition the programmer would need to add specific logging code:
	
	verbatim
	def function name(...) 
  if (DEBUG)
      println("function called at " + time());
      
      ...
	
	verbatim
	
	This behaviour is called an aspect (an area of a program which may be repeated several times which is unrelated to the purpose of the program). If the behaviour is to change (for example, changing the call to date() to a call to time() instead), each method declaration must be changed manually - a time consuming and potentially error-prone task.
	
	Instead, the use of aspects allows the programmer to remove this functionality, and combine a pointcut and advice into an aspect:
	
	verbatim
	def aspect TraceMethods 
  def pointcut method-call: execution.in(*) and not(flow.in(this));
		
  before method-call 
   println("function called at " + time());
  
	
	verbatim
	
	Although that from a software engineering perspective this is clearly a superior solution (decreases coupling, increases reuse, increases separation of concern), the use of aspects has not been widely adopted. There are several likely causes for this, such as:
	
	itemize
		Lack of education: like the other models that we have seen in section sec:introduction/parallelist, widespread adoption of new programming construct requires that the average programmer can understand the feature without in-depth education in the model. AOP is somewhat counter to intuitive definitions of imperative or procedural languages, which hampers their adoption.
		Lack of language support: no widely adopted programming language comes with AOP included, or with an AOP library included in the standard library. Standard licensing issues also apply to third-party additions (GPLv2/3 differences).
		Unclear flow control: perhaps the single largest issue with AOP. As noted by Constantinides2004, aspects introduce effectively unconditional branches into code, mimicking the use of goto which Dijkstra1968 famously considered harmful Dijkstra1968.
		Unintended consequences: defining aspects incorrectly can lead to incorrect (global) state, renaming methods and so on. If a team of developers are unaware of each other's modifications at weave-time, there may unintended consequences and subtle (or substantial) bugs introduced.
	itemize
	
	AspectJ/ABC sec:instrumentation/aop/aspectj
	AspectJ Kiczales2001 is an extension to the Java language that adds aspect-oriented features. It is a project of the Eclipse Foundation (of Eclipse IDE fame). The usage of AOP within Java is a somewhat natural extension as aspects can be seen as the modularisation of behaviour (concerns) over several classes - and not to forget that AOP was originally developed as an extension to object-oriented languages.
		
		Array and Loop Pointcuts sec:instrumentation/aop/aspectj/arrayloop
		However, the limitations of the AspectJ join-point model are somewhat obvious for this project. To be specific, `vanilla' AspectJ cannot define point cuts for neither array accesses or loops - a combination of which would be required for this project. In addition, the vanilla AspectJ implementation is not particularly extensible, which means that defining new point-cuts is somewhat difficult.
		
		There is, however, an implementation of AspectJ which is designed to be more extensible and compatible (mostly) with the original AspectJ implementation - abc, the AspectBench Compiler for AspectJ Allan2005.
		
		Although abc itself does not include point-cuts for either array access or loops, there exists two projects which, if combined, could offer the required features for this project.
		
		LoopsAJ Harbulot2005 is an extension to abc that adds a loop join point. This is not a trivial addition - when loops are compiled, they are compiled to forms that loose loop semantics (and instead use goto instructions). There are several forms that a loop can take, and a significant proportion of Harbulot2005's work is in the identification of loops from the bytecode.
				
		For array access, the ArrayPT project Chen2007 adds additional array access capabilities to abc. Although the included point cut does include array access, it is somewhat limited and cannot determine either the index, nor the value to be stored. ArrayPT adds these capabilities to abc. ArrayPT defines two new point cuts, arrayset(signature) and arrayget(signature). ArrayPT relies on the invokevirtual bytecode in the JVM.
		
		It is anticipated that, if these projects are combined, it would present a feasible approach to instrumentation.
		
		
		

Hybrid Models sec:instrumentation/hybrid
	DiSL sec:instrumentation/hybrid/disl
	Recently, there has been renewed interest in Java bytecode instrumentation. Clearly, the use of aspect-oriented techniques is advantageous, but the current implementations (AspectJ/abc) are deeply flawed. In a sense, they are static - they rely on predefined join and point-cuts before any aspect definitions can be constructed.
	
	DiSL (Domain Specific Language for Instrumentation) Marek2012 is a new approach to a domain-specific language (which incidentally, implies that DiSL is declarative) for bytecode instrumentation. It does rely on the use of aspects, but it instead uses an open join-point model where any area of bytecode can be instrumented. DiSL has several advantages over ApsectJ, including:
	
	itemize
		Lower overheads
		Greater expressibility of aspect and join-point definition
		Greater code coverage
		Efficient synthetic local variables for data exchange between join-points
	itemize
	
	Marek2012 present benchmarks of overheads with DiSL versus AspectJ, and their results are promising - a factor of three lower overheads, yet DiSL manages greater code coverage than AspectJ (the number of join-points captured is greater).
	
	Bytecode Instrumentation  Graal Transformations sec:instrumentation/hybrid/graalbytecodeIntroduction chp:introduction
Background sec:introduction/background
Ever since the introduction of the first microprocessors in the early 1970s, there has been a trend within the microprocessor industry affectionately called Moore's Law. Although not a law in the proper scientific sense (rather, it is more of an observation of the behaviour of the industry), it does accurately describe the trend of the number of transistors which can be placed in a given area Which is emphatically not the idea that processor speed doubles every 18 months - which is a common misconception. Moore's Law is also applicable to other VLSI products, such as memory.. The trend so far has been that this number double every 18 months. Altogether, Moore's Law successfully described the behaviour of the semiconductor industry until roughly five years ago.

Golden Age sec:introduction/golden-age
During this time of rapid advancement, programmers had to expend very little effort in order to improve performance of their programs on newer hardware. In the best-case scenario, literally no change was required whatsoever - not even a recompilation of the program. The underlying hardware was improving, and when one combines this fact with the separation of concern between user-level applications and the underlying hardware (i.e., the abstraction layer that compilers introduce, with a special focus on ISA abstraction) meant that developers could simply urge their users to buy new hardware for a performance improvement.

In a slightly less-than-idea scenario, the higher transistor counts being allowed would allow semiconductor designers to add new features to the `base' instruction set of their choice - for example on x86 there have been several additions over the years (MMX, 3DNow!, SSE, PAE, x86-64 etc). In these cases, programmers would simply need to recompile their programs with a compiler that would take advantage of the new extensions. Platforms supporting just-in-time (JIT) compilation such as Java, C etc would need to replace existing virtual machines (VMs) with ones capable of using the new instruction sets.

In many ways, this time could be seen as a golden age of computer architecture. Transistors were cheap and plentiful and the promise was always there that next year transistors would be even cheaper and more plentiful. Semiconductor manufacturers started experimenting with radical new designs (not all of which were successful, for example Intel's NetBurst which promised speeds of up to 10GHz by, amongst other techniques, involved utilising an extremely long pipeline). Consumers were confident that a new machine would be significantly faster than the machine they purchased a mere 12 months prior. Enabled by the new-found performance of processors, application developers would start to introduce many new layers of abstraction (and indirection), which would allow for safer, stabler programs to be written using high-level languages such as Ruby, Python, Perl and PHP. These extremely-high-level (EHL) languages (sometimes called scripting languages) commonly sacrificed execution speed for programmer ease of use, safely, new features and other such advantages. Indeed, this phenomenon even became widespread in lower-level languages via Java and C, both of which introduced a virtual machine between the application and the hardware. In many cases, these virtual machines were specifically designed (at least initially) for the languages for which they were designed (in that they were not initially designed to be `language agnostic'), meaning they may have allowed features that are difficult to implement lower in the stack. For example, the Java Virtual Machine (JVM) includes opcodes such as invokespecial (which calls a special class method), instanceof and other such codes specifically designed for an object-oriented language There is currently an effort to add new instructions to the JVM designed to ease execution of languages with non-object-oriented paradigms. These features are enabled via high-performance processors, and would likely not exist (or certainly, not be mainstream) without these processors.

Cheating the System sec:introduction/cheating
However, these increases cannot occur indefinitely. There exists not only a fundamental lower-bound on the size of an individual transistor (as a result of quantum tunnelling), but also the extent to which contemporary techniques can provide performance improvements. For example, many common processors exploit instruction-level parallelism (ILP) by executing several instructions at the same time - pipelining. This is achieved by effectively duplicating many stages of the pipeline and the supporting infrastructure. Besides the standard issues with pipelining (data, control and structural hazards spoiling issue flow, multi-cycle instructions spoiling commit flow and the like which can be solved via trace caches, as done in the Pentium 4), there exists a larger problem. As the degree to which ILP is exploited in a processor increases, the complexity of the supporting infrastructure increases combinatorially. Hence, this is clearly not the `silver bullet' which ILP was once thought to be. The extent to which current processors exploit ILP are not likely to increase significantly in the next several years, barring a revolutionary breakthrough in processor manufacturing, ILP detection/exploitation etc.

About a decade ago, it was a commonly held belief that the path to improving processor performance was to make a single-core processor increasingly powerful, through a combination of higher clock speeds (which manifested itself as the so-called `Megahertz War') and architectural improvements. Although this did come true to an extent (eventually culminating in the 3.8GHz Intel Pentium 4), this period did not yield the kind of performance that was expected (see above). The main reason for this was a simple one - transistors with higher switching frequencies produce more heat. This, when combined with the fact that Moore's Law would allow higher transistor counts per unit area meant that around 2006 to 2007 manufacturers were unable to improve performance much more simply through increasing the clockspeed.

Hello, Parallelism sec:introduction/parallelism
There existed no simple solution to this problem. For decades developers were used to having to expend little to no effort to realise potentially significant performance improvements. The solution that industry converged upon was that of parallelism - to improve performance not by increasing the performance of a single processor, but to provide many processors each of which are slightly slower when taken individually. When combined together (with a multi-threaded program), the culmination of these processors would be more performant than a single processor could ever be.

Parallelism (and concurrency) was not a new idea. For decades parallelism had been used for the most compute-intensive problems (such as ray tracing and scientific computing). These kinds of problems are usually `embarrassingly parallel' - each unit of work is totally independent from all other pieces of work. Example of this include ray-tracing, where each ray can be simulated independently; rasterisation, where each pixel can be computed in parallel and distributed scientific problems such as SETI@Home. Indeed, concurrency has been part of developers standard toolkit for many years since the advent of GUIs. In Java developers commonly use helper classes such as SwingWorker to run compute-intensive GUI tasks in a thread independent from UI event processing in order to prevent the UI `hanging' when performing long-running computations.

However the level of parallelism present in most applications is fairly superficial. Even using tools such as SwingWorker does not introduce a significant level of parallelism. For example, imagine a button that invokes a SwingWorker which executes a loop for many iterations. Although that loop is running on a different thread, that loop is still executing sequentially. A significant performance improvement could be realised if the developer had introduced structures and  processes that allow the loop to be executed in parallel; unfortunately these transformations are non-trivial and hence are usually not performed.

Regardless of the main reason that parallelism hasn't been introduced to any significant degree in programs (i.e., there was not a pressing need to), there are still many barriers to introducing parallelism. The main problem is likely that most developers simply do not have the required education or experience to do so. Parallelism and concurrency introduces many subtle timing errors that appear transiently. Scheduling algorithms are usually non-deterministic, which makes reasoning about them (either formally or informally) difficult. The behaviour of multi-threaded programs can change with varying number of processors.

Parallelist Approaches sec:introduction/parallelist
One solution to this problem that has been advocated by many theoretical computer scientists and language designers is to switch language paradigm to functional languages such as Haskell and Erlang. Languages and systems of this class do have significant advantages - both theoretical and applied - over more conventional paradigms. Purely functional languages have first-class, referentially-transparent functions which can be easily reasoned about by both the user and the compiler. However, here are two main disadvantages to this approach. Although it is the most optimal from a theoretical perspective (and mainstream switching to a functional language would bring many benefits, not just increased parallelism), it would require rewriting existing programs in a functional language. Moreover, most developers do not have experience with functional languages - and are not, unfortunately, willing to learn. It is not a problem of technology - more one of education.

Moving more towards the practical side of things, there are two main alternatives. The first is to use language-level constructs which are built into the semantics of the language. There has been some work in this area, and they usually focus on extending existing languages with parallel semantics. These additions usually provide language-level constructs for message passing, parallel loop construction, barriers and other such low-level parallelism primitives. Although there are many such research languages (Click-5, Chapel, X10 etc), there exists no commonly used language supporting these features. There are also some hybrid languages, such as Lime which are backwards compatible with existing infrastructures. The advantages of these languages is that because they support parallelism primitives intrinsically, reasoning and inference (both by humans and in an automated fashion) is much easier than it is for other languages. Adding support for parallelism at the language level also, depending on the level of abstraction used, implies tying a language to a particular parallel paradigm (or set of paradigms). If a language implements low-level constructs instead of higher-level constructs (similar to algorithmic skeletons except at the language level), then the user has to implement many commonly used operations manually, leading to bugs and inconsistencies between implementations. The education issue is also present with language-level constructs. Despite these disadvantages, some progress has been made in this area by Microsoft with PLINQ (Parallel Language Integrated Query), which is a declarative extension to CLI languages. PLINQ is successful because it requires little to no changes from standard LINQ - this is the ideal scenario in many cases (although note that PLINQ is not a general-purpose solution).

A more pragmatic approach to language-level constructs is a library-based approach such as POSIX Threads (Pthreads), OpenMP and OpenMPI. Pthreads is an implementation of the POSIX (Portable Operating System Interface) standard regarding threads and is therefore compatible with a wide variety of hardware and operating systems (including some non-POSIX compliant systems such as Windows). OpenMP is a library for C, C++ and Fortran for shared-memory programming, although it is commonly used to implement parallel loops (i.e., work sharing). OpenMPI is similar to OpenMP with the exception that it is designed for distributed-memory programming instead. The advantage of a library-based approach is that users can `mix-and-match' between different libraries - for example, a user can use both OpenMP and OpenMPI within the same program. However, many of the libraries require significant low-level knowledge of parallelism, and are not particularly user friendly.

To illustrate the substantial difficulties of adding parallel semantics to existing programs with sequential semantics, let us consider the challenges programmers face when using parallel features in programming languages, such as threads. Mutual exclusion is, currently, the most widely adopted parallel programming paradigm, as it is the easiest to use (although it does not nessecarily offer the greatest performance; nevertheless it can offer performance greater than that of sequential programs). Some common issues include Fraser04,Herlihy1993,ppls:

itemize lst:parallelhard
	Deadlock: where two or more threads are waiting on each other to finish before continuing 
	
	Livelock: similar to deadlock, except that the threads are changing state yet not achieving work
	
	Priority inversion: when a thread with high priority is pre-empted by a thread with a lower priority
	
	Race conditions: the result of two or more threads attempt to access the same resource(s) at the same time, leading to non-deterministic behaviour
itemize

There have been some attempts to mitigate these issues (Liu2011), but they have not yet been widely adopted.

Many web-scale companies (Facebook, Google, Yahoo! etc) have large datasets that require (offline) processing and are now using parallel infrastructures such as Hadoop (MapReduce) to support this computation. Such infrastructures are similar to library-based solutions except that they also provide management solutions and other such features. Hadoop, along with its sister projects HDFS, Hive and HBase, provide an entirely framework for programming, executing, distributing and managing parallel applications. An `all-in-one' solution such as Hadoop is extremely attractive simply because it provides everything one needs to set up distributed/parallel applications. The disadvantage is that, in general, such frameworks are only suited to a single kind of parallel framework. In the case of Hadoop, it can only support MapReduce-based computations. In other words, all computations that are not based around a MapReduce model are incompatible with Hadoop. Additionally, such frameworks typically require a large number of resources in order to be effective (Hadoop may actually reduce performance of computations if the number of processors is small), although that disadvantage is not inherent to MapReduce-style computations.

Perhaps the `holy grail' of parallelism is the idea of an auto-parallelising compiler. In other words, a compiler that can infer enough information about the semantics of the program in order to apply transformations that convert a program with sequential semantics into one with parallel semantics. This approach sounds extremely attractive, as it would not require (ideally) any effort on the behalf of the programmer; despite this there are significant disadvantages. Firstly, given the scope and context of contemporary common languages (C, C++, Objective-C, C, Java), applying compile-time transformations to add highly context-sensitive parallel semantics is likely in intractable problem. The reason for this is that, with current compiler technology, it is not possible to infer enough information regarding the semantics and syntax of the languages to reason about them. To illustrate this point, even when additional parallel semantics are added to existing languages (with sequential semantics), loops may still not be easily parallelisable because of pointer aliasing For example, imagine a function that returns an array of values which cannot be determined at compile-time. If that array is then used to index another array (say in a vector addition), reasoning about the parallel semantics of that vector addition are intractable at compile-time.

The last kind of parallelism is hardware-supported parallelism through constructs such as transactional memory (TM). TM solutions have the advantage of being easily programmable (to the extent that they are as easy as marking a method as synchronized, similar to how Java's monitors operate i.e., coarse-grained parallelism) whilst retaining the performance of fine-grained parallelism. The disadvantage of this approach is that high-performance TM architectures are reliant on hardware support (although software-based approaches do exist, they typically exhibit significantly lower performance characteristics than a similar hardware-based approach). Additionally, simply adding TM into a program does not add parallel semantics to a program with sequential semantics - constructs that allow e.g. threads to be created still need to be added. TM can rather be seen as of a way to make parallel programming easier, not a complete solution.

Transactional memory is, in effect, an optimistic memory model in that multiple threads attempt several transactions at the same time, and any conflicting writes are rolled fine. The final operation to be performed in a transaction is a commit operation - where the changes are recognised permanently in global state. However, unlike database management systems which are concerned with the ACID properties [p. 14]DatabasesBook:

itemize
	Atomicity: the `all-or-nothing' execution of transactions
	Consistency: no transaction can move global state from a consistent state to an inconsistent state
	Isolation: the fact that each transaction must appear to be executed as if no other transaction is executing at the same time
	Durability: the condition that the effect on global state of the transaction must never be lost once the transaction is complete
itemize

However, transactional memory when used in the context of computer systems (TM can be implemented in both hardware and software, but as the semantics are the same we shall consider only transactional memory `in the large'), we are principally concerned with only atomicity and isolation, as we assume that changes to memory do not need to be durable (memory operations are transient) Marshall2005.

Another way of utilising hardware support in parallelism is to make use of low-level constructs such as compare-and-swap, test-and-set (and test-and-test-and-set) and so on. These mechanisms are very simple, but they allow programmers to implement more complicated and sophisticated parallelism constructs using them. For example, the test-and-test-and-set operation can be used to implement locks in the following way:

verbatim
boolean function test-and-set(boolean v) 
< boolean initial = v;
v = true;
return initial;
>  


lock_t lock = false;
co [int i = 1 to n] 
while (something) 
 while (lock  test-and-set(lock))
     ;
     
 critical-section();
 lock = false;
 non-critical-section();


verbatim

Note that any code within  and  occurs atomically, and the co notation refers to  threads executing the block in parallel.

However, such approaches are equivalent to using a low-level language such as assembly or C to write a modern application - they are simply too low-level for programmers and are highly error prone.

It is interesting to note that languages with theoretical, rather than pragmatic, roots display excellent characteristics for automatic parallelisation. For example, functional languages display characteristics such as referential transparency. Referential transparency is the property that a function is composed entirely of pure functions - functions that do not modify global state. The advantage of this, along with the other properties of functional languages, is that it allows the compiler to reason about the program. Functions that display referential transparency lend themselves to easy automatic parallelisation.

Indeed, this idea of higher-level languages displaying good parallelisation characteristics extends to languages other than just functional languages. Many declarative languages (where the programmer describes what he/she wants to happen, in a sense without expressing control flow) lend themselves to automatic parallelisation, because they allow the compiler (or interpreter) to reason about the language. Declarative languages are semantically a `purer expression' of computation in the sense that they do not contain implementation details, which allows this reasoning by the compiler.

Language Semantics and Compiler Reasoning sec:introduction/reasoning
Referential transparency is advantageous for parallelisation because it limits shared state. The reason that the techniques listed above such as semaphores, mutexes and so on is to limit concurrent access to shared state variables. As referential transparency specifically disallows shared state, the advantages are clear. To give a concrete example of this, consider the following code, noting that function  is declared as referentially transparent:

verbatim
int transparent function f(int x, int y) 
    return x * y;


results = int array[10000][10000]

for int i = 0 to 10000 
    for int j = 0 to 10000 
        results[i][j] = f(i, j)
    

verbatim

Because  has been declared as transparent, the compiler can perform aggressive optimisations - for  processors, computing  iterations of the outer loop on each processor. It must be emphasised that the specific runtime configuration is not important - it is up to the compiler or runtime to determine the appropriate execution strategy. In theory, the compiler or runtime could even execute the loops on a more specialised device, such as a GPU.

So far in this chapter we have justified the use of declarative languages over imperative/procedural languages for parallelisation because they allow the compiler to reason about the semantics of the language PeytonJones2011. But what does it mean for a compiler to `reason' about a language? To answer this, let us consider some examples.

Consider the following SQL statement:

verbatim
SELECT
    username, firstname, lastname
FROM
    Users
WHERE
    id = 3
verbatim

As we know, SQL is a declarative language. The user has no control over the way that the SQL compiler/optimiser/interpreter/runtime executes the query, and this is precisely the power that declarative languages offer. Research has shown that optimistic optimistic compilers for declarative languages can, in some cases, out-perform hand-tuned C code Anderson2013,Mainland2013.

Declarative languages have already successfully been applied to several different area of parallel computing Orchard2010,Grossman2011,Holk2011.






Experimental Setup sec:methodology/setup
The hardware used for the experiments is a mid-2009 MacBook Pro with the following specifications:
	
itemize
	Intel Core 2 Duo P8800 @ 2.66GHz
	8GB 1067MHz DDR3
	Nvidia GeForce 9600M GT 256MB
	Disks:
	itemize
		Samsung SSD 830 Series 128GB via SATA-2
		Western Digital Scorpio Blue 1TB via SATA-2
	itemize
itemize

The software configuration is as follows:
itemize
	OS X 10.8.4 (build 12E55)
	Java 7 update 25
itemize

All software used was the latest version available at the time of writing.

Repeats sec:methodology/repeats

Benchmarks sec:methodology/benchmarks
	Validation and Basic Testing sec:methodology/benchmarks/simple
	
	Parametric Benchmarks sec:methodology/benchmarks/parametric
	
	Graph Processing Algorithms sec:methodology/benchmarks/graphs
	
	Java Grande sec:methodology/benchmarks/grande
	
	Mandelbrot sec:methodology/benchmarks/mandelbrot
	
Measurement Methodology sec:methodology/measurements
	Execution Time sec:methodology/measurements/time
	
	Memory Usage sec:methodology/measurements/memory
	The difficulties of measuring memory usage in Java programs due to the non-deterministic nature of the garbage collector are well documented in the literature Kim2000,Ogata2010. Despite this, the Java 7 API presents several techniques RuntimeDocs of measuring memory within the JVMRelated Work chp:related
Parallelising Compilers sec:related/compilers
The idea of an automatic parallelising compiler is not a particularly new one, and indeed has been the focus of much research since the dawn of structured programming with Fortran Backus1979.
	
Programs and/or runtime systems are attractive because they require no access to the source code. In Yang2011, Yang2011 introduced one of the main advances on the field, Dynamic Binary Parallelisation. It requires no access to the source code, and instead operates only using object code. The mechanism through which it operates is by detecting hot loops -- loops where the program spends a majority of execution time -- and parallelises them, executing the parallel versions speculatively. This speculation is likely the cause of the inefficiencies of their approach - using 256 cores they achieved a somewhat negligible performance improvement of just 4.5 times. One of the advantages of the approach presented in this dissertation is that it does not require the use of speculative execution - a loop is only executed if it can be proven to contain no inter-iteration dependencies.
	
The main difference between Yang2011's work and the work outlined here is the nature of the dynamic detection. Yang2011 used dynamic trace analysis, which can only identify the hot loops within a program, and not whether the iterations within those loops have dependencies. The work presented in this dissertation does determine whether there are inter-loop dependencies, therefore the use of speculative execution is not required. Although we have not yet done so, the addition of hot-loop detection would be a trivial addition to our framework.

Parallelism Detection sec:related/detectionResults chp:results
Validation sec:results/validation
[1]

figure
	
	[terminal=pdf]../graphs/memory.gp
	Hello, world!
figure

[2-6]

Basic Testing sec:results/basic

Parametric sec:results/parametric

Graph Processing sec:results/graph

Java Grande sec:results/grande

Mandelbrot sec:results/mandelbrot

Overhead sec:results/overhead
	Execution Time sec:results/overhead/time
	
	Memory Usage sec:results/overhead/memory

Analysis sec:results/analysisThe Runtime Library sec:runtime
In this chapter, the work performed towards implementing the runtime library is presented. This library handles several core functions critical to the infrastructure of the application:

itemize
	Functions for collecting trace analyses
	Implementations of trace storage backends
	Algorithms for offline and online dependency analysis
itemize

Additionally, tools for generating sample dependency patterns are also included.

The programs described within this chapter are open-source, released under The University of Edinburgh GPL license. They are available at https://github.com/chrisatkin/locomotion.

The fully-qualified package identifier for the runtime library is
uk.ac.ed.inf.icsa.locomotion.instrumentation.

Trace Collection sec:runtime/trace-collection
One of the main functionalities of the runtime library is to enable the collection of traces. This entails logging all appropriate memory operations (i.e., the ones matching conditions presented in items:reqs), with some additional semantic information which is also required.

The user-facing functions for trace collection have the following signatures:

verbatim
public static <T> void
arrayLookup(T[] array, int index, int iterator, int id);

public static <T> void
arrayStore(T[] array, int index, T value, int iterator, int id);
verbatim

The arguments are:

itemize items:trace-args
	T[] array: the array upon which the operation is occuring
	int index: the array index in question
	int iterator: the value of the iteration variable
	T value: the value being written to the array at the index
	int id: a loop identifier
itemize

The Java generics system was leveraged in order to reduce the amount of code required to implement the collection; it is important to recognise that the Java type system allows the trace collection mechanism to be unconcerned with the type of the array being accessed (and the type of the item being inserted if appropriate).

Trace Storage sec:runtime/storage
Generating traces for large programs -- the kind of programs which would benefit from hot-loop analysis -- requires a large amount of storage as it scales linearly with the number of memory operations (). Although the number of storage operations conforming to the requirements in a program may be relatively small, this number is increased when the standard library is included.
	
	Exact Approaches sec:runtime/storage/exact
	Exact approaches provide an accurate deterministic response to the 

	Probabilistic Approaches sec:runtime/storage/probabilistic
	The main disadvantage to using exact approaches is that the storage required scales linearly such that . For anything but the most trivial programs, this means that it becomes infeasible to store traces for all memory operations.
	
		Bloom Filters sec:runtime/storage/probabilistic/bloom
		One alternative is the use of Bloom Filters Bloom1970. A Bloom Filter is a randomised data structure which supports membership queries, with the possibility of false positives. In the context of parallelism detection, this means that we may conclude that a loop is not parallelisable when in reality, it is.
		
		The operation of a Bloom Filter is simple: there exists a bit vector of size  and a number  of hash functions (which could use universal hashing Carter1979). Upon insertation of an item , for each hash  the value of  is computed, which is an integer in the range . The corresponding index of the vector is then set to .
		
		figure[h!]
				
				graphics/bloomfilter.pdf
				Bloom filter operation with  and 
				fig:bloom-filter
		figure
			
		To test membership for an item, feed the item to each hash function. If all the corresponding indexes are , then the item may be contained within the filter - if any of them are , then the item is definitely not.
		
		For a given number of entries  and a bit vector of size , we need to use  hash functions such that:
		
		
		k = ms 2
		
		
		The error rate is defined as:
		
		
		= 0.5^k
		
		
		In essence, the longer the bit vector, the more accurate the filter becomes - at the expense of increasing space requirements. If , then there are no false positives - the filter becomes accurate and deterministic (for a fixed set of ).
		
		Swamidass2007 show is that the number of elements within a Bloom Filter can be estimated by:
		
		
		E = -n 1-n/nk
		

Dependency Analysis Algorithms sec:runtime/analysis
	Computing dependency between two operations is of critical importance to this project; an efficient algorithm is important.
	
	The problem statement for dependency analysis is as follows:
	
	Let  be the set of memory accesses for iteration  in loop . For a memory access , determine whether there exists a previous iteration such that  for some integer . Additionally, for an access  and previous access , .

	Offline Algorithms sec:runtime/analysis/offline
	An offline algorithm means that any processing is performed after the trace has been collected. It is the simplest form of dependency analysis algorithm, but it show poor performance. For a number of loops  with an average of  iterations each, where each iteration has an average of  operations, then .
	
	The algorithm is as follows:
	
	algorithm
		offline dependency algorithm
		alg:offline-dependency
		algorithmic[1]
			 to 
			carry out some processing
	 		
		algorithmic
	algorithm

	Online Algorithns sec:runtime/analysis/online

  
  [2][]
    (gnuplot) 
      Package color not loaded in conjunction with
      terminal option `colourtext'
    See the gnuplot documentation for explanation.
    Either use 'blacktext' in gnuplot or load the package
      color.sty in LaTeX.
    



      Package graphicx or graphics not loaded
    See the gnuplot documentation for explanation.
    The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.
    
























      
      LTw
      LTb
      LTa
      LT0
      LT1
      LT2
      LT3
      LT4
      LT5
      LT6
      LT7
      LT8
    
      
      
      
      LTw
      LTb
      LTa
      LT0
      LT1
      LT2
      LT3
      LT4
      LT5
      LT6
      LT7
      LT8
    
  
  
  



















































