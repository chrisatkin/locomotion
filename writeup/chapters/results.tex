\chapter{Results} \label{chp:results}
In this chapter, the results of the various experiments that were conducted are presented and analysed. The experimental procedure described in chapter \ref{chp:methodology} was followed.

The results are split into several sections. Section \ref{sec:results/bfimpl} presents the experiments with regard to the optimal selection of bloom filter and hash set implementation. Then, section \ref{sec:results/precision} analyses the precision of the chosen bloom filter implementation, and draws conclusions to hash sets. Section \ref{sec:results/memory} analyses the memory usage required for both schemes. Finally, section \ref{sec:results/time} compares the execution time required for each scheme with a break down into `raw' overhead costs (ie, where no work is performed per access) and `real-world' costs, where a defined number of computations occur per memory operation.

\section{Bloom Filter Implementation} \label{sec:results/bfimpl}
As shown previously, the main overhead for bloom filters is the cost of computing the $k$ hashes required for their operation. It is well-documented that hash functions have a relatively high computational time complexity, and so the cost of hashing in a bloom filter is quite high. Reducing this time complexity is therefore crucial to improving the performance of the bloom filter. In this section, we perform an analysis of the implementation currently used, against another implementation in order to evaluate the importance of the hashing algorithms.

The first comparison that we make is to simply determine the raw overhead of insertion of values. This will allow us to determine an upper bound of the expected performance if the mechanism was inserted into the instrumentation framework. Two operations were benchmarked, the insertion operation and the membership operation. We measured for a number of accesses ranging 1000 to 100,000 (in steps of 1000), the median execution time for each operation for each of the storage formats. Note that the data was not collected on a `rolling' basis, for each length an entirely new experiment was run. We considered the following storage structures:

\begin{itemize}
	\item A \texttt{HashSet} with a predefined size of $n$, where $n$ is the number of accesses and a load factor of 0.75
	\item A \texttt{HashSet} with default settings (\ie, an initial length of 16 and a load factor of 0.75)
	\item The aforementioned Google Guava implementation of bloom filter, with insertions set to $n$ and a false positive probability of 0.03
	\item Lastly, the Cassandra bloom filter implementation with the same settings as the Guava implementation
\end{itemize}

	\subsection{Insertion} \label{sec:results/bfimpl/insert}
	The insertion operation is the process of adding an object into the data structure in question. In both tests, the load factor of the hash set was 0.75, as recommended by the Java documentation.
	
	\begin{figure}
		\centering
		\begin{gnuplot}[terminal=pdf]
			load '../dynamic/formatted-results/implementation/insert-local.gnuplot'
		\end{gnuplot}
		\caption{Median insertion costs for each data structure tested. (Notice the use of log scale on the y axis)}
		\label{chart:impl-insert}
	\end{figure}
	
	As we can see from figure \ref{chart:impl-insert}, the Cassandra implementation of bloom filters carries a significant overhead when compared to all other structures, with roughly a 50\% higher execution time than the Guava bloom filter. This can be attributed to the aforementioned (section \ref{sec:runtime/storage/probabilistic/impls}) pre-requisite `stringification' required for this implementation.
	
	The structure with the highest performance is clearly a hash set with a capacity set to the number of iterations. Unlike bloom filters, hash sets use only one hash function, whereas bloom filters use $k$. This, when combined with an optimal number of buckets results in significantly higher performance than the other structures.
	
	When the hash set is not optimally configured (\ie, initial capacity set to the default of 16), it performs on average at the same performance as the Guava bloom filter. This is because whilst the bloom filter requires $k$ hash functions, the set will often require rehashing in order to increase the underlying data structures.
	
	The initial spikes in execution time followed by a 'levelling out' of the times are mainly a result of the JIT compiler - initially, the interpreter is executing the code. When the JIT recompiles to native code, performance increases significantly. There may also be some additional factors, such as caches still being initialized.
	
	\subsection{Membership} \label{sec:results/bfimpl/member}
	
	\begin{figure}
		\centering
		\begin{gnuplot}[terminal=pdf]
			load '../dynamic/formatted-results/implementation/contains-local.gnuplot'
		\end{gnuplot}
		\caption{Median membership query costs for each data structure tested. (Notice the use of log scale on the y axis)}
		\label{chart:impl-member}
	\end{figure}
	
	Figure \ref{chart:impl-member} presents the results of the membership query testing.
	
	As with insertion costs, the stringification required for Cassandra has a significant impact on the execution time.
	
	The results for the other structures largely mirror those for insertion. The optimally configured hash set performs best, with a roughly 20 to 30\% performance improvement over both the Guava bloom filter and a default hash set. Again, this is to be expected as a result of the lower time complexity associated with the optimal hash set.
	
	Overall, we observe that execution times for both operations are similar. There appears to be no additional overhead for insertion above the overhead required for membership querying.
	
	\begin{figure}
		\centering
		\begin{gnuplot}[terminal=pdf]
		set multiplot layout 1,2
			load '../dynamic/formatted-results/vector-survey-all/bloomfilter-deps.gnuplot'
			load '../dynamic/formatted-results/vector-survey-none/bloomfilter-deps.gnuplot'
		unset multiplot
		\end{gnuplot}
		\caption{Precision results for all dependent and none dependent}
		\label{chart:precision-all}
	\end{figure}
	
	\subsection{Conclusions} \label{sec:results/bfimpl/conclusions}
	From these results, we can immediately disregard the Cassandra bloom filter for further testing, as it has been optimised for strings and, although it may be used as one, it is not a general-purpose bloom filter. This is unlike the Guava implementation, which is both high-performance \emph{and} general-purpose.
	
	This high-performance is a result of the design of the Guava implementation. In addition to using funnels instead of other forms of serialisation (see section \ref{sec:runtime/storage/probabilistic/impls}), it also employs a mathematical `trick' presented in \citet{Kirsch2006} in order to reduce the amount of hashing required below that of a standard bloom filter -- although we must emphasis that $k >= 2$ (\ie, there are still more hash functions required than hash sets).
	
	For future testing, we will use the Guava implementation for bloom filters, as well as optimally-configured hash sets, as these showed the highest performance characteristics. Note that our choices here cannot impact the precision/accuracy of future experiments. Although we did not consider accuracy in these experiments, both bloom filters are capable of attaining 100\% accuracy; we were simply seeking the implementation with the lowest overhead.
	
\section{Precision Testing} \label{sec:results/precision}

\begin{figure}
	\centering
	\begin{gnuplot}[terminal=pdf]
	set multiplot layout 1,2
		load '../dynamic/formatted-results/multiples-all/bloomfilter-deps.gnuplot'
		
		load '../dynamic/formatted-results/multiples-none/bloomfilter-deps.gnuplot'
	unset multiplot
	\end{gnuplot}
	\caption{Precision rates for various factors of iteration counts}
	\label{chart:precision-factors}
\end{figure}

Now that we have chosen the optimally-performing bloom filter and configured hash set, we can now consider the accuracy which can be attained through their use.

For space reasons, we present figures for 100\% dependencies and 0\% dependencies only. Figures for other dependency rates (20\%, 40\%, 60\% and 80\%) are available in appendix \ref{chp:appndx:precision-figs}.

Figure \ref{chart:precision-all} presents the precision results for the chosen structures. The charts show both the advantages and disadvantages of using bloom filters - in some cases, they were as precise as hash sets.

In both cases, the error rate decreased logarithmically as the number of expected insertions increased, as fitting with the theoretical results presented in section \ref{sec:runtime/storage/probabilistic/impls}.

We can also see that the bloom filter hashing mechanism is indeed evenly distributed. This is evidence through when the number of expected insertions equals the number of actual insertions, the bloom filter correctly determines that there are no dependencies.

The reason that the number of detected dependencies increases to the value that it does is the nature of the detection. As mentioned previously, there are two bloom filters used per loop - one for reads, and one for writes. However, because there are only three kind of dependencies, upon a store operation both filters must be checked, whereas the loads filter must only be checked one - a total of three checks (we note that each dependency type has equal weighting). As each filter becomes saturated, they report a 100\% dependency rate, which results in 3x higher dependency detection rate above the actual.

\begin{figure}
	\centering
	\begin{gnuplot}[terminal=pdf]
		set multiplot layout 1,2
			load '../dynamic/formatted-results/vector-survey-all/hash-memory.gnuplot'
			load '../dynamic/formatted-results/vector-survey-all/bloomfilter-memory.gnuplot'
	\end{gnuplot}
	\caption{Memory usage for all dependent, hash set and bloom filter}
	\label{chart:mem-hashbloom-all}
\end{figure}

Generally speaking, the bloom filters were as accurate as the hash set when the expected insertion count was roughly 10 times higher than the number of actual insertions. In order to test this hypothesis, we ran several more experiments where instead of using pre-determined bit vector lengths (as in these experiments), we instead used lengths of various multiples of the number of accesses. These results are presented in figure \ref{chart:precision-factors}.

As we can see, when the expected insertions is increased to a factor, the results are much more promising. We can see that for all factors, the precision for all dependent was 100\% (\ie, all operations were detected with no false positives).

Although the case is similar for no dependencies, the were two false positives detected at 500,000 iterations and four detected at 600,000 iterations when the factor was 10. Increasing the factor above 10 resulted in no false positives. This again highlights the possible disadvantage of using a probabilistic data structure.

We note that although it is not possible to reduce the false positive probability rate to 0\% in bloom filters, we can modify the parameters such that they can be minimized. Modifying the expected insertions (and hence, the bit vector length) is, in our case, superior to modifying the false positive probability rate since doing so modifies the number of hash functions required, therefore increasing execution time.

In conclusion, we observe that when the number of expected iterations is set to 20 times greater than the number of actual insertions, bloom filters offer accuracy equal to that of hash sets. Although the false positive probability rate cannot become zero, the use of sufficiently large bit vector lengths can mitigate the risk substantially enough that there is effectively no difference (in terms of detection rates) between bloom filters and hash sets.

\section{Memory Usage} \label{sec:results/memory}
We next consider the memory usage of each approach.

\begin{figure}
	\centering
	\begin{gnuplot}[terminal=pdf]
		load '../dynamic/formatted-results/multiples-all/bloomfilter-memory.gnuplot'
	\end{gnuplot}
	\caption{blah}
	\label{chart:mem-factors}
\end{figure}

Figure \ref{chart:mem-hashbloom-all} presents the memory usage, in kilobytes, versus the number of accesses when all operations are dependent. Note that the memory usage for all other configurations of dependencies is the same, and hence not shown here.

There are several observations which can be made immediately. The first is that the required memory for hash set is significantly higher than that for bloom filters. It is somewhat difficult to compare directly between the two mechanisms since memory usage increases as a function of number of accesses for hash sets (and indeed, we see that the result is the expected linear increase), whilst as a function of expected insertions (and not actual insertions) for bloom filters. Despite this, we can see that for 100,000 accesses, the hash set required roughly 12MB, whilst the corresponding bloom filter with 100,000 expected insertions (which gained a 100\% accurate rate), required just 200KB - a factor of 60 improvement.

\begin{figure}
	\centering
	\begin{gnuplot}[terminal=pdf]
	set multiplot layout 1,2
		load '../dynamic/formatted-results/vector-survey-all/time.gnuplot'
		
		load '../dynamic/formatted-results/multiples-all/bloomfilter-time.gnuplot'
	unset multiplot
	\end{gnuplot}
	\caption{Execution time for synthetic benchmark}
	\label{chart:time-all}
\end{figure}

As expected, the memory usage for bloom filters also follows a linear relationship: double the number of expected insertions and the memory requirement also doubles.

We now consider the case when multiples are taken into consideration; figure \ref{chart:mem-factors} shows this. We also include hash set for comparison purposes.

We observe that the memory required for factors increases substantially, in some cases it is above that required for hash sets. The this occurs when the multiple is set to 70 times or higher. When we take this, and the preciseness of both methods, we can conclude that these is no net benefit to using factors greater than 70. However, given the results presented in section \ref{sec:results/precision}, we can \emph{also} conclude that it is not necessary to use a bit vector of above a factor of 20.

When a multiple of 10 is used, memory usage for 1,000,000 accesses increases to 20MB. This is considerably lower than that required for hash sets, which is 11MB, leading to an improvement of 5.5x lower memory usage. If the same analysis is performed except with a factor of 20, the improvement decreases to roughly 2.75x.

\section{Execution Time} \label{sec:results/time}
\begin{figure}
	\centering
	\begin{gnuplot}[terminal=pdf]
	set multiplot layout 1,2
		load '../dynamic/formatted-results/vector-survey-all/time-noinstr.gnuplot'
		
		load '../dynamic/formatted-results/vector-survey-none/time-noinstr.gnuplot'
	unset multiplot
	\end{gnuplot}
	\caption{Execution time for synthetic without instrumentation}
	\label{chart:time-none-noinstr}
\end{figure}

We next consider the case of execution time within a synthetic environment. In other words, the impact on execution time when no other operations are taken into account.

From figure \ref{chart:time-all}, we can clearly see the impact of instrumentation, with there being little difference between the times recorded without instrumentation. Slight fluctuations are expected as a result of experimental noise.

We can also observe the impact of using a bloom filter over a hash set, with execution times required for bloom filters being roughly 20 to 40\% higher than that those required for hash set.

This can be attributed to the additional cost of $k$ hash functions (leading to time complexity $T=O(k)$ as opposed to $T=O(1)$ for the hash set). With the additional hash requirements of the instrumentation (as explained in section \ref{sec:results/memory}) leads to a higher overhead for execution. Additionally, this result has been found by previous authors.

When we consider the relative execution times for the various bloom filter configurations, we see that as the error rate increases, so does the execution time. This can be explained by the mechanism for reporting dependencies. When a dependency is detected, an exception is thrown. Although this provides the caller with semantic information regarding the dependency (possibly allowing further statistic to be computed), the overhead of throwing exceptions is high, as the runtime must walk the stack performing pattern matching as to when the exception can be caught. This results in the additional overhead of inadequate bloom filter configurations; increasing evidence for the need of well-configured bloom filters.

Figure \ref{chart:time-none-noinstr} shows the same programs being run without instrumentation. As we can see, without instrumentation any fluctuation is minimal, with what little there is being explained by noise in the results (and it is not significant).

Execution time increases as a function of accesses, independent of any other variables; there is a highly linear relationship.

We can observe that with this synthetic benchmark, instrumentation increases execution times between a factor of 20 and 30 times. Although quite large, this is a relatively strong result for instrumented programs, which are often hundreds \citep{Uh2006}, \citetext{priv.\ comm.}.

\section{Real Impact} \label{sec:results/real}
\begin{figure}
	\centering
	\begin{gnuplot}[terminal=pdf]
	set multiplot layout 1,2
		load '../dynamic/formatted-results/optimal-all-computation/time-bloom.gnuplot'
		
		load '../dynamic/formatted-results/optimal-all-computation/time-noinstr.gnuplot'
	unset multiplot
	\end{gnuplot}
	\caption{Execution times for bloom filters and no instrumentation when all operations are dependent}
	\label{chart:real-time-bloomnoinstr}
\end{figure}

In this section, we consider the `real-world' impact of instrumentation, as opposed to the benchmarks in section \ref{sec:results/time}, as those benchmarks did not perform any additional computation.

In these experiments, we tested the optimal bloom filter configuration (20x multiple) with operations that were 100\% and 0\% dependent. The variable in this experiment was the number of floating-point iterations per memory operation.

As we can see from charts \ref{chart:real-time-bloomnoinstr} and \ref{chart:real-time-hashnoinstty}, the addition of computation results in significant time increases, and the time required for the computation becomes the dominant factor. In other words, the overhead required as a result of the instrumentation is relatively negligable compared to that required for the computation. As a result, we can calculate that the overhead of the instrumentation if computation is included results in execution times of between a factor of 1.5 and 2 times greater than the uninstrumented programs. This is true for both hash sets and bloom filters, with the difference being negligible between the two.

\begin{figure}
	\centering
	\begin{gnuplot}[terminal=pdf]
	set multiplot layout 1,2
		load '../dynamic/formatted-results/optimal-none-computation/time-bloom.gnuplot'
		
		load '../dynamic/formatted-results/optimal-none-computation/time-noinstr.gnuplot'
	unset multiplot
	\end{gnuplot}
	\caption{Execution times for bloom filters and no instrumentation when no operations are dependent}
	\label{chart:real-time-hashnoinstty}
\end{figure}

\section{Performance Tuning} \label{sec:results/tuning}
