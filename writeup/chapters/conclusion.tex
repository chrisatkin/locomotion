\chapter{Conclusion} \label{chp:conclusion}
\section{Concluding Remarks and Contributions} \label{sec:conclusion/remarks}
In this dissertation, several key contributions to the field have been made.

\begin{itemize}
	\item We have investigated the use of bloom filters as an alternative to hash sets for dependency storage. Despite their probabilistic nature, we have shown that is is both feasible and advantageous to use bloom filters instead of hash sets for dependency analysis. Bloom filters significantly outperformed hash sets when memory usage is taken into consideration, and they outperform hash sets by roughly 10 to 15\% when execution time is taken into consideration.
	
	\item A parametric benchmark has been developed, which enabled the creation of access pattern with well-known properties. This allows for `apples-to-apples' comparisons to be made between various different approaches. Additionally, the use of this benchmark allows for a controlled setting within which the benchmarks can take place -- one can only evalulate the efficacy of dependency detection algorithms if the number of dependencies is known. Unlike other (perhaps real-world) benchmarks where this value may not be known, with our benchmark it is known.
	
	\item Lastly, an instrumentation framework with an overhead of 20-30x over the baseline has been developed. We have shown how this framework performs with regard to the number of computations performed with each access.
\end{itemize}

We can draw the final conclusion that it is both possible and advantageous to use bloom filters in the pursuit of an automatic parallelising runtime. The use of bloom filters allows for both a negligible increase in memory size (on the order of tens of megabytes versus hundreds for hash sets) and a relatively small increase in execution times of between a factor of 1.5 and 2 times slower than uninstrumented programs if a full dependency analysis is sought. Low overhead for instrumentation is crucial; \citet{EdlervonKoch2013} showed that even with modest increases in execution time can significantly impact the potential speedup as a result of dynamic parallelisation.

In the case of a binary decision regarding parallelisation (\ie, `can this loop be executed in parallel or not?'), the speedup is potentially even larger; in our experiments the performance is comparable to that of uninstrumented programs.

We found that the optimal size for the bloom filter is 20 times the number of actual insertions. This allows for the optimal trade-off between dependency detection (which was 100\% accuracy for 20x in our testing) and memory usage, which was a factor of 3 lower than that required for hash sets.

Finally, we found that varying the false positive probability rate in order to modify the number of hash functions had little effect on decreasing the execution time. This shows conclusively that the use of bloom filters carries a higher overhead than that of an (optimally-configured) hash set.

\section{Recommendations} \label{sec:conclusion/recommend}
As a result of this work, we can support the use of bloom filters as an alternative to hash sets for trace collection and analysis. We found the Guava bloom filter implementation to be the highest performing, with $fpp=0.07$ offering the optimal trade-off between execution time overhead and memory usage. When using bloom filters for this purpose, users should set the number of expected insertions to 15 to 20 times higher than the number of actual insertions.

\section{Unsolved Problems} \label{sec:conclusion/unsolved}
There are, however, several problems with remain with our implementation. These are mainly in the form of the lack of automatic instrumentation. We attempted to use the Graal compiler infrastructure to add these instructions, but unfortunately Graal does not yet support them. The largest issue was that in the case of a deoptimisation occuring, the interpreter does not have a bytecode index to commence execution from because the instruction does not exist within the bytecode itself.

In the coming weeks and months, the Graal development team will be adding new features and capabilities and soon will support this feature. Although we are currently unsure of the form this will take, it will in all likelihood be in a form that is easily compatible with our framework. Indeed, if it takes the form that we expect, no modifications will be required to the existing solution.

\section{Future Work} \label{sec:conclusion/future-work}
The work that has been presented in this dissertation is a step forwards in dynamic parallelism detection. The framework correctly identifies data-parallel loops, and we have investigated whether the use of bloom filters affects the detection rate. However, there are still significant areas of future work that are possible.

Additional methods for trace storage could be analysed. Perhaps one such example is hash compaction, which instead stores compacted states in a hash table.

The framework presented is only currently capable of detecting parallelism at the level of loops (\ie, loop-level parallelism). The approach used could be extended towards a slicing-based approach, where instead of loops being instrumented, it could be possible to instrument given blocks. Whilst the approach is feasible, the framework would need to be modified in the sense that it can currently only instrument array accesses. Although a simple addition as the underlying mechanisms would not need to be modified, only their presentation API, this work is outside the scope of this dissertation. Additionally, such an approach would face the same problems as \citet{Wang2009} - as the slice coverage increases, the complexity increases combinatorially. 

Additionally, although the framework \emph{does} correctly detect parallelism at run-time, this information is current under-exploited by the runtime. It is possible, at least theoretically, that the framework could be combined with fellow student Ranjeet Singh's Java-to-OpenCL compiler, or perhaps utilise the already existing PTX or HSAIL backends in Graal in a JIT setting to produce a runtime system that dynamically detects parallel loops, recompiles then and executes using OpenCL (either on a CPU or GPU). However, there is an additional downside to this: the advantage (\ie, performance increase) of dynamic recompilation, moving execution to the CPU and then gathering the result must be greater than that of not doing so.

There are several overheads of doing so however. Although thread creation on CPUs is expensive in Java \citep{JSR133}, thread creation on GPUs is low \citep{Mueller2009} (indeed, GPUs need 1000s of threads to operate efficiently in CUDA \citep{Nvidia2011}).

Additionally, the overhead of data marshalling for GPGPUs (in general terms, the overhead of moving the data to the GPU and back again if required) is high, as the data does not exist within the cache hierarchy. In order to overcome this overhead, the advantage of performing the optimisation must be significant. A cost model could be performed, similar to \citet{Tournavitis2009}, which combines static and dynamic analysis in order to provide intelligent run-time information to the JIT compiler regarding possible parallelisation for loops.

There also exists another possibility of a storage backend based on bloom filters, which could possibly decrease memory usage further, although at the expense of higher execution times. Currently, we propose using a single, large bloom filter of a multiple of the expected number of insertions. However, it is possible to instead use a small, fixed size bloom filter instead, and upon insertion testing the current false positive probability rate. If the fpp is above a threshold, a new bloom filter is created and used instead. For membership queries, one needs to iterate over all $f$ bloom filters, each of which have $k$ hash functions (as they are all configured the same), resulting in an increased execution time of $T=O(fk)$. The execution time for insertion remains the same. Using such a scheme, excess memory usage could be kept to a minimum. This technique would also have the advantage of being able to use a bloom filter when the number of insertions is not known in advance.